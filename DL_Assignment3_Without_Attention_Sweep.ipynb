{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2vkvje2XA3r"
      },
      "source": [
        "# Import package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-05-13T17:56:45.220189Z",
          "iopub.status.busy": "2022-05-13T17:56:45.219611Z",
          "iopub.status.idle": "2022-05-13T17:56:59.642417Z",
          "shell.execute_reply": "2022-05-13T17:56:59.641258Z",
          "shell.execute_reply.started": "2022-05-13T17:56:45.220142Z"
        },
        "id": "u0qCvgqBxzLo",
        "outputId": "2d06e0a6-9706-4fcf-f615-bd7218ca42de",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.16)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.12)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.3)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.9)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Collecting xtarfile\n",
            "  Downloading xtarfile-0.1.0.tar.gz (3.4 kB)\n",
            "Building wheels for collected packages: xtarfile\n",
            "  Building wheel for xtarfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for xtarfile: filename=xtarfile-0.1.0-py3-none-any.whl size=3862 sha256=c1909bc809cfce32b17924e074c2c4c7f07bfdf4d7f5daddabb85036c5d818c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/ed/96/b06b7600d9d2eb6068fed8ded4282aeb3addf5b213dfe48819\n",
            "Successfully built xtarfile\n",
            "Installing collected packages: xtarfile\n",
            "Successfully installed xtarfile-0.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "!pip install xtarfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:56:59.646057Z",
          "iopub.status.busy": "2022-05-13T17:56:59.645631Z",
          "iopub.status.idle": "2022-05-13T17:56:59.654209Z",
          "shell.execute_reply": "2022-05-13T17:56:59.653024Z",
          "shell.execute_reply.started": "2022-05-13T17:56:59.646002Z"
        },
        "id": "6xAqFEssB1RJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "START_TOKEN=\"\\t\"\n",
        "END_TOKEN=\"\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:56:59.661908Z",
          "iopub.status.busy": "2022-05-13T17:56:59.659308Z",
          "iopub.status.idle": "2022-05-13T17:57:07.051305Z",
          "shell.execute_reply": "2022-05-13T17:57:07.050263Z",
          "shell.execute_reply.started": "2022-05-13T17:56:59.661854Z"
        },
        "id": "s6xB0KDuy95e",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import wandb\n",
        "import re, string\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.font_manager import FontProperties\n",
        "from collections import Counter\n",
        "import os\n",
        "from os.path import exists\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import random \n",
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import csv\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xJ8lIJvgitRW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import random \n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from os.path import exists\n",
        "import xtarfile as tarfile\n",
        "import pandas as pd \n",
        "import keras\n",
        "START_TOKEN=\"0\"\n",
        "END_TOKEN=\"1\"\n",
        "\n",
        "\n",
        "def downloadDataSet():\n",
        "   cwd = os.getcwd()\n",
        "  \n",
        "   file_exists = exists('./dakshina_dataset_v1.0.tar')\n",
        "   if(file_exists==False):\n",
        "     print('downloading....')\n",
        "     os.system('curl -SL https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar > dakshina_dataset_v1.0.tar')\n",
        "     print('download Complete')\n",
        "   extract_exists = exists('./dakshina_dataset_v1.0/')   \n",
        "   if(extract_exists==False): \n",
        "     print('Extracting..') \n",
        "     with tarfile.open('dakshina_dataset_v1.0.tar', 'r') as archive:\n",
        "         archive.extractall()\n",
        "     print('Complete')\n",
        "   print('You are all set')\n",
        "def get_files(language):\n",
        "\n",
        "  train_dir='./dakshina_dataset_v1.0/'+language+'/lexicons/'+language+'.translit.sampled.train.tsv'\n",
        "  val_dir='./dakshina_dataset_v1.0/'+language+'/lexicons/'+language+'.translit.sampled.dev.tsv'\n",
        "  test_dir='./dakshina_dataset_v1.0/'+language+'/lexicons/'+language+'.translit.sampled.test.tsv'\n",
        "  \n",
        "  return train_dir, val_dir, test_dir\n",
        "\n",
        "def tokenize(lang,tokenizer=None):\n",
        "    \"\"\" Uses tf.keras tokenizer to tokenize the data/words into characters\n",
        "    \"\"\"\n",
        "    if(tokenizer==None):\n",
        "        tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "        tokenizer.fit_on_texts(lang)\n",
        "        lang_tensor = tokenizer.texts_to_sequences(lang)\n",
        "        lang_tensor = tf.keras.preprocessing.sequence.pad_sequences(lang_tensor,\n",
        "                                                            padding='post')\n",
        "    else:\n",
        "  \n",
        "        lang_tensor = tokenizer.texts_to_sequences(lang)\n",
        "        lang_tensor = tf.keras.preprocessing.sequence.pad_sequences(lang_tensor,\n",
        "                                                        padding='post')\n",
        "\n",
        "    return lang_tensor, tokenizer\n",
        "def preprocess_data(fpath,ip_tokenizer=None, tgt_tokenizer=None):\n",
        "   \n",
        "    #Read data from files\n",
        "    df = pd.read_csv(fpath, sep=\"\\t\", header=None)\n",
        "\n",
        "    #Add start and end token\n",
        "    df[0] = df[0].apply( lambda x:START_TOKEN+x+END_TOKEN) \n",
        "    ip_tensor, ip_tokenizer = tokenize(df[1].astype(str).tolist(), tokenizer=ip_tokenizer)\n",
        "    \n",
        "    tgt_tensor, tgt_tokenizer = tokenize(df[0].astype(str).tolist(), tokenizer=tgt_tokenizer) \n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices((ip_tensor, tgt_tensor))\n",
        "    dataset = dataset.shuffle(len(dataset))\n",
        "    \n",
        "    return dataset, ip_tokenizer, tgt_tokenizer\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:07.053772Z",
          "iopub.status.busy": "2022-05-13T17:57:07.053448Z",
          "iopub.status.idle": "2022-05-13T17:57:07.068478Z",
          "shell.execute_reply": "2022-05-13T17:57:07.066675Z",
          "shell.execute_reply.started": "2022-05-13T17:57:07.053724Z"
        },
        "id": "d1J0s1-UsYaC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "\"\"\"\n",
        "Bahdanau Attention\n",
        "\"\"\"\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, enc_state, enc_out):\n",
        "    \n",
        "    enc_state = tf.concat(enc_state, 1)\n",
        "    enc_state = tf.expand_dims(enc_state, 1)\n",
        "\n",
        "    score = self.V(tf.nn.tanh(self.W1(enc_state) + self.W2(enc_out)))\n",
        "\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    context_vector = attention_weights * enc_out\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:07.07297Z",
          "iopub.status.busy": "2022-05-13T17:57:07.072345Z",
          "iopub.status.idle": "2022-05-13T17:57:07.906438Z",
          "shell.execute_reply": "2022-05-13T17:57:07.905226Z",
          "shell.execute_reply.started": "2022-05-13T17:57:07.072904Z"
        },
        "id": "ls7v013HipOJ",
        "outputId": "fd6f158d-7360-4579-80b2-afb2ee01d5cf",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:07.935325Z",
          "iopub.status.busy": "2022-05-13T17:57:07.93476Z",
          "iopub.status.idle": "2022-05-13T17:57:07.95351Z",
          "shell.execute_reply": "2022-05-13T17:57:07.952408Z",
          "shell.execute_reply.started": "2022-05-13T17:57:07.935278Z"
        },
        "id": "ty-yWidXsJyD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class Parameters():\n",
        "  def  __init__(self,  language='te',encoder_layers=1,decoder_layers=1,embedding_dim=128,\\\n",
        "                layer_type='lstm', units=128, dropout=0.5, attention=False,attention_type=\"Luong\",batch_size=128,\\\n",
        "                apply_beam_search=False,apply_teacher_forcing=False,teacher_forcing_ratio=1,\\\n",
        "                 save_outputs=None,epochs=5,wandb=None,beamWidth=5,restoreBestModel=True,\\\n",
        "                 patience=2,encoder_vocab_size=64,decoder_vocab_size=64):\n",
        "        self.language = language\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.encoder_layers=encoder_layers\n",
        "        self.decoder_layers=decoder_layers\n",
        "        self.layer_type = layer_type\n",
        "        self.units = units\n",
        "        self.dropout = dropout\n",
        "        self.attention = attention\n",
        "        self.stats = []\n",
        "        self.wandb=wandb\n",
        "        self.epochs=epochs\n",
        "        self.batch_size = 128\n",
        "        self.apply_beam_search = apply_beam_search\n",
        "        self.batch_size = batch_size\n",
        "        self.apply_teacher_forcing=apply_teacher_forcing\n",
        "        self.save_outputs=save_outputs\n",
        "        self.restoreBestModel=restoreBestModel\n",
        "        self.attention_type=attention_type\n",
        "        self.patience=patience\n",
        "        self.encoder_vocab_size=encoder_vocab_size\n",
        "        self.decoder_vocab_size=decoder_vocab_size\n",
        "        self.teacher_forcing_ratio=teacher_forcing_ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:07.956076Z",
          "iopub.status.busy": "2022-05-13T17:57:07.955483Z",
          "iopub.status.idle": "2022-05-13T17:57:07.97555Z",
          "shell.execute_reply": "2022-05-13T17:57:07.97435Z",
          "shell.execute_reply.started": "2022-05-13T17:57:07.956029Z"
        },
        "id": "KQSzzTVAsEX0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\"\"\"This class contain all funtion to add Encoder layers\n",
        "Input: Param \n",
        "This variable contain all configuration details.But most focus goes on following attributes:\n",
        "- layer_type\n",
        "- encoder_layers\n",
        "- units\n",
        "- dropout\n",
        "\"\"\"\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, param):\n",
        "        #Configurations\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layer_type = param.layer_type\n",
        "        self.n_layers = param.encoder_layers\n",
        "        self.units = param.units\n",
        "        self.dropout = param.dropout\n",
        "        self.embedding = tf.keras.layers.Embedding(param.encoder_vocab_size, param.embedding_dim,trainable=True)\n",
        "\n",
        "        #Create Recurrant Layers\n",
        "        self.create_rnn_layers()\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        x = self.rnn_layers[0](x, initial_state=hidden)\n",
        "\n",
        "        #Get returned output and state value\n",
        "        for layer in self.rnn_layers[1:]:\n",
        "            x = layer(x)\n",
        "\n",
        "        output, state = x[0], x[1:]\n",
        "\n",
        "        return output, state\n",
        "    \"Create Encoder layer\"\n",
        "    def create_rnn_layers(self):\n",
        "        self.rnn_layers = []\n",
        "        #Add one or more encoder layers\n",
        "        for i in range(self.n_layers):\n",
        "            self.rnn_layers.append(get_layer(self.layer_type, self.units, self.dropout,\n",
        "                                                return_sequences=True,\n",
        "                                                return_state=True))\n",
        "\n",
        "\n",
        "    def initialize_hidden_state(self, batch_size):\n",
        "\n",
        "        if self.layer_type != \"lstm\":\n",
        "            return [tf.zeros((batch_size, self.units))]\n",
        "        else:\n",
        "            return [tf.zeros((batch_size, self.units))]*2\n",
        "\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:07.981314Z",
          "iopub.status.busy": "2022-05-13T17:57:07.980762Z",
          "iopub.status.idle": "2022-05-13T17:57:08.004816Z",
          "shell.execute_reply": "2022-05-13T17:57:08.003792Z",
          "shell.execute_reply.started": "2022-05-13T17:57:07.981262Z"
        },
        "id": "NQHJ8JtysOCX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "\"\"\"This class contain all funtion to add Decoder layers\n",
        "Input: Param \n",
        "This variable contain all configuration details.But most focus goes on following attrib\n",
        "- layer_type\n",
        "- encoder_layers\n",
        "- units\n",
        "- dropout\n",
        "- Attention [True,False]\n",
        "-  attention_type [Luong,Bahdanau]\n",
        "\"\"\"\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self,param):\n",
        "\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        #Basic configurations\n",
        "        self.layer_type = param.layer_type\n",
        "        self.n_layers = param.decoder_layers\n",
        "        self.units =param. units\n",
        "        self.dropout = param.dropout\n",
        "\n",
        "        #Following configuration useful in case of attention enabled model\n",
        "        # attention_type = [Luong,Bahdanau]\n",
        "        self.attention = param.attention\n",
        "        \n",
        "        self.attention_type=param.attention_type\n",
        "\n",
        "        #Add embedding layers \n",
        "        self.embedding_layer = layers.Embedding(input_dim=param.decoder_vocab_size, \n",
        "                                                output_dim=param.embedding_dim,trainable=True)\n",
        "        \n",
        "        self.dense = layers.Dense(param.decoder_vocab_size, activation=\"softmax\")\n",
        "        self.flatten = layers.Flatten()\n",
        "\n",
        "        #Verify If want to add attention layers \n",
        "        #it will be  Bahdanau attention\n",
        "    \n",
        "        if self.attention:\n",
        "\n",
        "            self.attention_layer = BahdanauAttention(self.units)\n",
        "\n",
        "        #Add one/more recurrant layers based on confugurations\n",
        "        self.create_rnn_layers()\n",
        "\n",
        "    def call(self, x, hidden, enc_out=None):\n",
        "        #Add embedding input layer\n",
        "        x = self.embedding_layer(x)\n",
        "\n",
        "        #Verify if attention layer need to be added\n",
        "        if self.attention:\n",
        "            context_vector, attention_weights = self.attention_layer(hidden, enc_out)\n",
        "            x = tf.concat([tf.expand_dims(context_vector, 1), x], -1)\n",
        "        else:\n",
        "            attention_weights = None\n",
        "\n",
        "        \n",
        "        x = self.rnn_layers[0](x, initial_state=hidden)\n",
        "\n",
        "        #Get returned output and state value\n",
        "        for layer in self.rnn_layers[1:]:\n",
        "            x = layer(x)\n",
        "\n",
        "        output, state = x[0], x[1:]\n",
        "\n",
        "        output = self.dense(self.flatten(output))\n",
        "        \n",
        "        return output, state, attention_weights\n",
        "    #Create decoder layesr\n",
        "    def create_rnn_layers(self):\n",
        "        self.rnn_layers = []   \n",
        "\n",
        "        #Add one or more decoder layers\n",
        "\n",
        "        for i in range(self.n_layers - 1):\n",
        "            self.rnn_layers.append(get_layer(self.layer_type, self.units, self.dropout,\n",
        "                                                return_sequences=True,\n",
        "                                                return_state=True))\n",
        "        \n",
        "        self.rnn_layers.append(get_layer(self.layer_type, self.units, self.dropout,\n",
        "                                            return_sequences=False,\n",
        "                                            return_state=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:08.007545Z",
          "iopub.status.busy": "2022-05-13T17:57:08.007003Z",
          "iopub.status.idle": "2022-05-13T17:57:08.101039Z",
          "shell.execute_reply": "2022-05-13T17:57:08.100005Z",
          "shell.execute_reply.started": "2022-05-13T17:57:08.007496Z"
        },
        "id": "Pa0XPzaFsSkW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from tqdm import tqdm\n",
        "import random \n",
        "import time \n",
        "import wandb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "class SequenceTOSequence():\n",
        "    def __init__(self, parameters):\n",
        "\n",
        "        #Basic configurations\n",
        "        self.param=parameters\n",
        "        self.embedding_dim = parameters.embedding_dim\n",
        "        self.encoder_layers = parameters.encoder_layers\n",
        "        self.decoder_layers = parameters.decoder_layers\n",
        "        self.layer_type = parameters.layer_type\n",
        "        self.units = parameters.units\n",
        "        self.dropout = parameters.dropout\n",
        "        self.batch_size = parameters.batch_size\n",
        "\n",
        "        #Add information regarding attention layer\n",
        "        self.attention = parameters.attention\n",
        "        self.attention_type = parameters.attention_type\n",
        "\n",
        "        self.stats = []\n",
        "      \n",
        "        self.apply_beam_search = parameters.apply_beam_search\n",
        "        \n",
        "        #Early stop conditions\n",
        "        self.patience=parameters.patience\n",
        "        self.restoreBestModel=parameters.restoreBestModel\n",
        "\n",
        "        #teacher forcing\n",
        "        self.apply_teacher_forcing=parameters.apply_teacher_forcing    \n",
        "        self.teacher_forcing_ratio=parameters.teacher_forcing_ratio\n",
        "\n",
        "    #Build model Add specific optimizers    \n",
        "    def build(self, loss, metric,optimizer='adam',lr=0.001):\n",
        "        self.loss = loss\n",
        "\n",
        "        #Select specific optimizer\n",
        "        if(optimizer=='adam'):\n",
        "          self.optimizer=tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "        if(optimizer=='nadam'):\n",
        "          self.optimizer=tf.keras.optimizers.Nadam(learning_rate=lr)\n",
        "        else:\n",
        "          self.optimizer=tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
        "         \n",
        "        self.metric = metric\n",
        "\n",
        "    def set_vocabulary(self, input_tokenizer, targ_tokenizer):\n",
        "        self.input_tokenizer = input_tokenizer\n",
        "        self.targ_tokenizer = targ_tokenizer\n",
        "        self.create_model()\n",
        "    \n",
        "    \"\"\"This procedure used to define Encoder Decoder Layer\"\"\"\n",
        "    def create_model(self):\n",
        "\n",
        "        encoder_vocab_size = len(self.input_tokenizer.word_index) + 1\n",
        "        decoder_vocab_size = len(self.targ_tokenizer.word_index) + 1\n",
        "        self.param.encoder_vocab_size=encoder_vocab_size\n",
        "        self.param.decoder_vocab_size=decoder_vocab_size\n",
        "        #Add Encoder layer\n",
        "\n",
        "        self.encoder = Encoder(self.param)\n",
        "\n",
        "        #Create decode with or without any attention layer\n",
        "        #Check following properties to add attention\n",
        "        # self.attention\n",
        "        # self.attention_type\n",
        "        self.decoder = Decoder(self.param)\n",
        "                \n",
        "\n",
        "    @tf.function\n",
        "    def train(self, input, target, enc_state):\n",
        "\n",
        "        loss = 0 \n",
        "\n",
        "        with tf.GradientTape() as tape: \n",
        "\n",
        "            enc_out, enc_state = self.encoder(input, enc_state)\n",
        "\n",
        "            dec_state = enc_state\n",
        "            dec_input = tf.expand_dims([self.targ_tokenizer.word_index[\"\\t\"]]*self.batch_size ,1)\n",
        "\n",
        "            apply_teacher_forcing=False\n",
        "            #decide whether to use teacher forcing\n",
        "            if random.random() < self.teacher_forcing_ratio:\n",
        "              apply_teacher_forcing=True\n",
        "            ## We use Teacher forcing to train the network\n",
        "            ## Each target at timestep t is passed as input for timestep t + 1\n",
        "            if  (apply_teacher_forcing==True):\n",
        "                #Apply teacher forcing\n",
        "                for t in range(1, target.shape[1]):\n",
        "\n",
        "                    preds, dec_state, _ = self.decoder(dec_input, dec_state, enc_out)\n",
        "                    loss += self.loss(target[:,t], preds)\n",
        "                    self.metric.update_state(target[:,t], preds)\n",
        "                    #As teacher forcing  applied we pass next target as decoder input\n",
        "                    dec_input = tf.expand_dims(target[:,t], 1)\n",
        "            \n",
        "            else:\n",
        "                #Without teacher forcing\n",
        "\n",
        "                for t in range(1, target.shape[1]):\n",
        "\n",
        "                    preds, dec_state, _ = self.decoder(dec_input, dec_state, enc_out)\n",
        "                    loss += self.loss(target[:,t], preds)\n",
        "                    self.metric.update_state(target[:,t], preds)\n",
        "                    #As teacher forcing not applied we pass decoder input as whatever we predict\n",
        "                    preds = tf.argmax(preds, 1)\n",
        "                    dec_input = tf.expand_dims(preds, 1)\n",
        "\n",
        "\n",
        "            batch_loss = loss / target.shape[1]\n",
        "\n",
        "            variables = self.encoder.variables + self.decoder.variables\n",
        "            gradients = tape.gradient(loss, variables)\n",
        "\n",
        "            self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "        return batch_loss, self.metric.result()\n",
        "\n",
        "    \n",
        "  \n",
        "\n",
        "    def fit(self, dataset, val_dataset, batch_size=128, epochs=5, wandb=None,apply_teacher_forcing=True, teacher_forcing_ratio=0.7):\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.apply_teacher_forcing = apply_teacher_forcing\n",
        "        self.teacher_forcing_ratio=teacher_forcing_ratio\n",
        "        #Prepare chunk of data based on batch size provided\n",
        "        steps_per_epoch = len(dataset) // self.batch_size\n",
        "        #steps_per_epoch_val = len(val_dataset) // self.batch_size\n",
        "        \n",
        "        dataset = dataset.batch(self.batch_size, drop_remainder=False)\n",
        "        #val_dataset = val_dataset.batch(self.batch_size, drop_remainder=False)\n",
        "\n",
        "      \n",
        "        sample_inp, sample_targ = next(iter(dataset))\n",
        "        self.max_target_len = sample_targ.shape[1]\n",
        "        self.max_input_len = sample_inp.shape[1]\n",
        "\n",
        "        #Store Encoder ,decoder details in case model get good accuracy\n",
        "        #Will be useful to restore best model\n",
        "        self.bestEncoder=self.encoder\n",
        "        self.bestDecoder=self.decoder\n",
        "        self.bestoptimizer=self.optimizer\n",
        "        \n",
        "        accuracyDegradePatience=0\n",
        "        self.oldaccuracy=0\n",
        "        for epoch in  tqdm(range(1, epochs+1), total = epochs,desc=\"Epochs \"):\n",
        "             \n",
        "            if(accuracyDegradePatience>=self.patience):\n",
        "                if(self.restoreBestModel==True):\n",
        "                    self.encoder=self.bestEncoder\n",
        "                    self.decoder=self.bestDecoder\n",
        "                    self.optimizer=self.bestoptimizer\n",
        "                break\n",
        "            ## Training loop ##\n",
        "            total_loss = 0\n",
        "            total_acc = 0\n",
        "            self.metric.reset_states()\n",
        "\n",
        "            starting_time = time.time()\n",
        "            enc_state = self.encoder.initialize_hidden_state(self.batch_size)\n",
        "\n",
        "            \n",
        "           \n",
        "            for batch, (input, target) in enumerate(dataset.take(steps_per_epoch)):\n",
        "                #Accumulate loss and accurecy for each batch\n",
        "                batch_loss, acc = self.train(input, target, enc_state)\n",
        "                total_loss += batch_loss\n",
        "                total_acc += acc\n",
        "            #Calculate validation accurecy for current Epoch\n",
        "          \n",
        "            avg_acc = total_acc / steps_per_epoch\n",
        "            avg_loss = total_loss / steps_per_epoch\n",
        "\n",
        "            # Validation loop ##\n",
        "            total_val_loss = 0\n",
        "            total_val_acc = 0\n",
        "            self.metric.reset_states()\n",
        "\n",
        "            enc_state = self.encoder.initialize_hidden_state(self.batch_size)\n",
        "\n",
        "            #Process data in batches\n",
        "            \n",
        "            avg_val_loss, avg_val_acc = self.evaluate(val_dataset,batch_size=self.batch_size)\n",
        "              \n",
        "\n",
        "           \n",
        "            #Verify if model performance degrading.add()\n",
        "            #In case train accurecy improved but no significant imprrovement in validation\n",
        "            #Add condition for early stopping\n",
        "            #Restore best model based on the input\n",
        "            if(self.oldaccuracy>avg_val_acc):\n",
        "              accuracyDegradePatience+=1\n",
        "            else:\n",
        "              self.bestEncoder=self.encoder\n",
        "              self.bestDecoder=self.decoder\n",
        "              self.bestoptimizer=self.optimizer\n",
        "              self.oldaccuracy=avg_val_acc\n",
        "              accuracyDegradePatience=0\n",
        "            print( \"\\nTrain Loss: {0:.4f} Train Accuracy: {1:.4f} Validation Loss: {2:.4f} Validation Accuracy: {3:.4f}\".format(avg_loss, avg_acc*100, avg_val_loss, avg_val_acc*100))\n",
        "            \n",
        "            time_taken = time.time() - starting_time\n",
        "\n",
        "            #Add logs for WanDb\n",
        "            self.stats.append({\"epoch\": epoch,\n",
        "                            \"train_loss\": avg_loss,\n",
        "                            \"val_loss\": avg_val_loss,\n",
        "                            \"train_acc\": avg_acc*100,\n",
        "                            \"val_acc\": avg_val_acc*100,\n",
        "                            \"training time\": time_taken})\n",
        "            \n",
        "            #Log to wanDB\n",
        "            if not (wandb is None):\n",
        "                wandb.log(self.stats[-1])\n",
        "            \n",
        "            print(f\"\\nTime taken for the epoch {time_taken:.4f}\")\n",
        "           \n",
        "        \n",
        "        print(\"\\nModel trained successfully !!\")\n",
        "    @tf.function\n",
        "    def validation(self, inp, trgt, encoder_state):\n",
        "        #Custom validation\n",
        "\n",
        "        loss = 0\n",
        "        #encoder input\n",
        "        encoder_output, encoder_state = self.encoder(inp, encoder_state)\n",
        "\n",
        "        #Set initial state of decoder from encoder state\n",
        "        decoder_state = encoder_state\n",
        "        decoder_input = tf.expand_dims([self.targ_tokenizer.word_index[\"\\t\"]]*self.batch_size ,1)\n",
        "\n",
        "        for t in range(1, trgt.shape[1]):\n",
        "            #Get decoder prediction\n",
        "            prediction, decoder_state, _ = self.decoder(decoder_input, decoder_state, encoder_output)\n",
        "            loss += self.loss(trgt[:,t], prediction)\n",
        "            self.metric.update_state(trgt[:,t], prediction)\n",
        "\n",
        "            prediction = tf.argmax(prediction, 1)\n",
        "            decoder_input = tf.expand_dims(prediction, 1)\n",
        "\n",
        "        batch_loss = loss / trgt.shape[1]\n",
        "        \n",
        "        return batch_loss, self.metric.result()    \n",
        "    def evaluate(self, test_dataset, batch_size=None):\n",
        "        \"\"\"Evaluate our model on test data\"\"\"\n",
        "        if batch_size is not None:\n",
        "            self.batch_size = batch_size\n",
        "\n",
        "        #prepare chuck of data based on the batch size\n",
        "        steps_per_epoch_test = len(test_dataset) // batch_size\n",
        "        test_dataset = test_dataset.batch(batch_size, drop_remainder=True)\n",
        "        \n",
        "        total_test_loss = 0\n",
        "        total_test_acc = 0\n",
        "        self.metric.reset_states()\n",
        "\n",
        "        enc_state = self.encoder.initialize_hidden_state(self.batch_size)\n",
        "\n",
        "        #print(\"\\nRunning test dataset through the model...\\n\")\n",
        "        #Run in batches based on the input batch size\n",
        "        for batch, (input, target) in enumerate(test_dataset.take(steps_per_epoch_test)):\n",
        "            batch_loss, acc = self.validation(input, target, enc_state)\n",
        "            total_test_loss += batch_loss\n",
        "            total_test_acc += acc\n",
        "\n",
        "        #Caculate avarage  test accuracy and loss\n",
        "        avg_test_acc = total_test_acc / steps_per_epoch_test\n",
        "        avg_test_loss = total_test_loss / steps_per_epoch_test\n",
        "\n",
        "        #Display details\n",
        "        #print(f\"Test Loss: {avg_test_loss:.4f} Test Accuracy: {avg_test_acc:.4f}\")\n",
        "\n",
        "        return avg_test_loss, avg_test_acc\n",
        "\n",
        "    \"\"\" This function used to translate english world to respective language\"\"\"\n",
        "\n",
        "    def translate(self, word, get_heatmap=False):\n",
        "        #start and end token for input word\n",
        "        start=\"\\t\"\n",
        "        end=\"\\n\"\n",
        "        word =start  + word + end\n",
        "\n",
        "        #Tokenize input and perform  preprocessing  \n",
        "        inputs = self.input_tokenizer.texts_to_sequences([word])\n",
        "        inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                               maxlen=self.max_input_len,\n",
        "                                                               padding=\"post\")\n",
        "\n",
        "        result = \"\"\n",
        "        att_wts = []\n",
        "\n",
        "        #Process input through encoder\n",
        "        enc_state = self.encoder.initialize_hidden_state(1)\n",
        "        enc_out, enc_state = self.encoder(inputs, enc_state)\n",
        "\n",
        "        # Set initial decoder sate to encoder state\n",
        "        dec_state = enc_state\n",
        "        dec_input = tf.expand_dims([self.targ_tokenizer.word_index[start]]*1, 1)\n",
        "\n",
        "        #Run the loop for maximum word size the target language can have\n",
        "        #We get this data during training \n",
        "        for t in range(1, self.max_target_len):\n",
        "\n",
        "            preds, dec_state, attention_weights = self.decoder(dec_input, dec_state, enc_out)\n",
        "            \n",
        "            #Add attention weights which is useful for generating attention heatmaps\n",
        "            if get_heatmap:\n",
        "                att_wts.append(attention_weights)\n",
        "            \n",
        "            #Pass the current prediction as input to next iteration\n",
        "            preds = tf.argmax(preds, 1)\n",
        "\n",
        "            #Accumulate target words\n",
        "            next_char = self.targ_tokenizer.index_word[preds.numpy().item()]\n",
        "            result += next_char\n",
        "\n",
        "            #Decoder input for next iteration\n",
        "            dec_input = tf.expand_dims(preds, 1)\n",
        "\n",
        "            #If we receive end token stop the loop\n",
        "            if next_char == end:\n",
        "                break\n",
        "\n",
        "        return result[:-1], att_wts[:-1]\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:08.103339Z",
          "iopub.status.busy": "2022-05-13T17:57:08.102802Z",
          "iopub.status.idle": "2022-05-13T17:57:08.113188Z",
          "shell.execute_reply": "2022-05-13T17:57:08.11221Z",
          "shell.execute_reply.started": "2022-05-13T17:57:08.103293Z"
        },
        "id": "rT-bKfIIWcES",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras import layers\n",
        "from tqdm import tqdm\n",
        "def get_layer(name, units, dropout, return_state=False, return_sequences=False):\n",
        "\n",
        "    if name==\"rnn\":\n",
        "        return layers.SimpleRNN(units=units, dropout=dropout, \n",
        "                                return_state=return_state,\n",
        "                                return_sequences=return_sequences)\n",
        "\n",
        "    if name==\"gru\":\n",
        "        return layers.GRU(units=units, dropout=dropout, \n",
        "                          return_state=return_state,\n",
        "                          return_sequences=return_sequences)\n",
        "\n",
        "    if name==\"lstm\":\n",
        "        return layers.LSTM(units=units, dropout=dropout, \n",
        "                           return_state=return_state,\n",
        "                           return_sequences=return_sequences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:08.115727Z",
          "iopub.status.busy": "2022-05-13T17:57:08.115041Z",
          "iopub.status.idle": "2022-05-13T17:57:08.125087Z",
          "shell.execute_reply": "2022-05-13T17:57:08.124108Z",
          "shell.execute_reply.started": "2022-05-13T17:57:08.115665Z"
        },
        "id": "6dS2JhY05vMR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import wandb "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:08.12831Z",
          "iopub.status.busy": "2022-05-13T17:57:08.12697Z",
          "iopub.status.idle": "2022-05-13T17:58:08.041134Z",
          "shell.execute_reply": "2022-05-13T17:58:08.039989Z",
          "shell.execute_reply.started": "2022-05-13T17:57:08.128261Z"
        },
        "id": "hLEZUp3W5wf9",
        "outputId": "69244e5f-b1d2-4efd-dc03-3acac1189983",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220514_171405-2vfsh9ri</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/kankan-jana/uncategorized/runs/2vfsh9ri\" target=\"_blank\">hopeful-dream-61</a></strong> to <a href=\"https://wandb.ai/kankan-jana/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/kankan-jana/uncategorized/runs/2vfsh9ri?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f3f681e1810>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCVhhqPMWkuS"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:58:08.044395Z",
          "iopub.status.busy": "2022-05-13T17:58:08.044039Z",
          "iopub.status.idle": "2022-05-13T17:58:09.136596Z",
          "shell.execute_reply": "2022-05-13T17:58:09.135546Z",
          "shell.execute_reply.started": "2022-05-13T17:58:08.044346Z"
        },
        "id": "p5Bt3Bf-_JhQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "START_TOKEN=\"\\t\"\n",
        "END_TOKEN=\"\\n\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWI9X48FjCPy",
        "outputId": "1dac3fdd-6108-43af-dc95-df55d0324eb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "downloading....\n",
            "download Complete\n",
            "Extracting..\n",
            "Complete\n",
            "You are all set\n"
          ]
        }
      ],
      "source": [
        "downloadDataSet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:58:09.13871Z",
          "iopub.status.busy": "2022-05-13T17:58:09.138399Z",
          "iopub.status.idle": "2022-05-13T17:58:10.220961Z",
          "shell.execute_reply": "2022-05-13T17:58:10.219885Z",
          "shell.execute_reply.started": "2022-05-13T17:58:09.138662Z"
        },
        "id": "6ji0OxT0yxRA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "language=\"te\"\n",
        "train_dir, val_dir, test_dir = get_files(language)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:58:10.22341Z",
          "iopub.status.busy": "2022-05-13T17:58:10.222458Z",
          "iopub.status.idle": "2022-05-13T17:58:17.153887Z",
          "shell.execute_reply": "2022-05-13T17:58:17.152529Z",
          "shell.execute_reply.started": "2022-05-13T17:58:10.223364Z"
        },
        "id": "vWOhUaVWipOU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset, input_tokenizer, targ_tokenizer = preprocess_data(train_dir)\n",
        "val_dataset, _, _ = preprocess_data(val_dir,input_tokenizer,targ_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rOIv0WNAipOU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:58:17.156262Z",
          "iopub.status.busy": "2022-05-13T17:58:17.155897Z",
          "iopub.status.idle": "2022-05-13T17:58:20.361771Z",
          "shell.execute_reply": "2022-05-13T17:58:20.360733Z",
          "shell.execute_reply.started": "2022-05-13T17:58:17.156216Z"
        },
        "id": "A57aPT6P6uyg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#train data \n",
        "dataset, input_tokenizer, targ_tokenizer = preprocess_data(train_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:58:40.546067Z",
          "iopub.status.busy": "2022-05-13T17:58:40.545721Z",
          "iopub.status.idle": "2022-05-13T18:01:06.858305Z",
          "shell.execute_reply": "2022-05-13T18:01:06.854753Z",
          "shell.execute_reply.started": "2022-05-13T17:58:40.546033Z"
        },
        "id": "KZwjhjH0ipOV",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlbXxZXZjpEq"
      },
      "source": [
        "## Sweep Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-13T17:58:21.748889Z",
          "iopub.status.idle": "2022-05-13T17:58:21.749784Z",
          "shell.execute_reply": "2022-05-13T17:58:21.749484Z",
          "shell.execute_reply.started": "2022-05-13T17:58:21.749451Z"
        },
        "id": "ikX_8MejzT7-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "  \"name\": \"DL_Assignment3_Rnn\",\n",
        "  \"method\": \"bayes\",\n",
        "  \"metric\": {\n",
        "      \"name\": \"val_acc\",\n",
        "      \"goal\": \"maximize\",\n",
        "  },\n",
        "  \n",
        "  \"parameters\": {\n",
        "        \"num_of_encoders\":{\n",
        "          \"values\":[1,2,3,4]    \n",
        "        },\n",
        "        \"num_of_decoders\":{\n",
        "            \"values\":[1,2,3]      \n",
        "\n",
        "        },\n",
        "        \"cell_type\":{\n",
        "          \"values\":['lstm','gru','rnn']  \n",
        "        },\n",
        "        \n",
        "        \"lr\":{\n",
        "          \"values\":[0.05,0.001,0.005]  \n",
        "        },\n",
        "        \"optimizer\":{\n",
        "          \"values\":['adam','rmsprop']  \n",
        "        },\n",
        "        \"dropout\":{ \"values\": [0.3,0.5]},\n",
        "        \"latent_dim\":{ \"values\": [64,128,256,512]},\n",
        "        \"inp_emb_size\": {\"values\": [64,128,256]},\n",
        "        \"batch_size\":{\"values\":[256,128,64]},\n",
        "        \n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 804
        },
        "execution": {
          "iopub.status.busy": "2022-05-13T17:58:21.751564Z",
          "iopub.status.idle": "2022-05-13T17:58:21.752441Z",
          "shell.execute_reply": "2022-05-13T17:58:21.752161Z",
          "shell.execute_reply.started": "2022-05-13T17:58:21.752127Z"
        },
        "id": "2dt9LTQAzZHh",
        "outputId": "02e336a6-198a-4486-8671-18846a4bae50",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f3f689a4750>> (for pre_run_cell):\n"
          ]
        },
        {
          "ename": "Exception",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36m_resume_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resuming backend\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublish_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_jupyter_teardown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface.py\u001b[0m in \u001b[0;36mpublish_resume\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpublish_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0mresume\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResumeRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface_shared.py\u001b[0m in \u001b[0;36m_publish_resume\u001b[0;34m(self, resume)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResumeRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRunRecord\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface_queue.py\u001b[0m in \u001b[0;36m_publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pb.Record\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_check\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The wandb backend process has shutdown\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: The wandb backend process has shutdown"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f3f689a4750>> (for post_run_cell):\n"
          ]
        },
        {
          "ename": "Exception",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36m_pause_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Attempt to save the code on every execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_ipynb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved code: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublish_pause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_attaching\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mlog_code\u001b[0;34m(self, root, name, include_fn, exclude_fn)\u001b[0m\n\u001b[1;32m    973\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfiles_added\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36m_log_artifact\u001b[0;34m(self, artifact_or_path, name, type, aliases, distributed_id, finalize, is_user_created, use_after_commit)\u001b[0m\n\u001b[1;32m   2551\u001b[0m                     \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinalize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2552\u001b[0m                     \u001b[0mis_user_created\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_user_created\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2553\u001b[0;31m                     \u001b[0muse_after_commit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_after_commit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2554\u001b[0m                 )\n\u001b[1;32m   2555\u001b[0m                 \u001b[0martifact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logged_artifact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LazyArtifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_public_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface.py\u001b[0m in \u001b[0;36mcommunicate_artifact\u001b[0;34m(self, run, artifact, aliases, history_step, is_user_created, use_after_commit, finalize)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhistory_step\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mlog_artifact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_artifact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface_shared.py\u001b[0m in \u001b[0;36m_communicate_artifact\u001b[0;34m(self, log_artifact)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_communicate_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_artifact\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogArtifactRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_artifact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_artifact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     def _communicate_artifact_send(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface_shared.py\u001b[0m in \u001b[0;36m_communicate_async\u001b[0;34m(self, rec, local)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_router\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_check\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The wandb backend process has shutdown\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_router\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_and_receive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: The wandb backend process has shutdown"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# This is the main function to use to train/fine-tune the model using wandb runs\n",
        "def train_wandb():\n",
        "    run = wandb.init()\n",
        "  \n",
        "    config=wandb.config\n",
        "    # Set the run name\n",
        "    name=\"num_of_encoders(\"+ str(config[\"num_of_encoders\"]) + \")_\"\n",
        "    name = \" num_of_decoders(\" + str(config[\"num_of_decoders\"]) + \")_\"\n",
        "    name += \" cell_type(\" + str(config[\"cell_type\"]) + \")_\"\n",
        "    \n",
        "    name += \"latent_dim(\" + str(config[\"latent_dim\"])+ \")_\"\n",
        "    name += \"lr(\" + str(config[\"lr\"])+ \")_\"\n",
        "    name += \"optimizer(\" + str(config[\"optimizer\"]) + \")_\"\n",
        "    name += \"dropout(\" + str(config[\"dropout\"]) + \")\"\n",
        "    name += \"inp_emb_size(\" + str(config[\"inp_emb_size\"]) + \")_\"\n",
        "    name+=\"batch_size(\" + str(config[\"batch_size\"]) + \")\"\n",
        "\n",
        "\n",
        "    wandb.run.name = name[:-1]\n",
        "    batch_size=config[\"batch_size\"]\n",
        "    inp_emb_size=config[\"inp_emb_size\"]\n",
        "    dropout=config[\"dropout\"]\n",
        "    optimizer=config[\"optimizer\"]\n",
        "    num_of_encoders=config[\"num_of_encoders\"]\n",
        "    num_of_decoders=config[\"num_of_decoders\"]\n",
        "\n",
        "    lr=config[\"lr\"]\n",
        "    latent_dim=config[\"latent_dim\"]\n",
        "    cell_type=config[\"cell_type\"]\n",
        "\n",
        "   \n",
        "    param=Parameters(language=\"te\",\\\n",
        "                        embedding_dim=inp_emb_size,\\\n",
        "                        encoder_layers=num_of_encoders,\\\n",
        "                        decoder_layers=num_of_decoders,\\\n",
        "                        layer_type=cell_type,\\\n",
        "                        units=latent_dim,\\\n",
        "                        dropout=dropout,\n",
        "                        epochs=15,\\\n",
        "                 batch_size=batch_size\\\n",
        "                   )\n",
        "    param.apply_teacher_forcing=True\n",
        "    param.teacher_forcing_ratio=1\n",
        "    param.patience=5\n",
        "    model = SequenceTOSequence(param)\n",
        "    model.set_vocabulary(input_tokenizer, targ_tokenizer)\n",
        "\n",
        "    model.build(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\\\n",
        "                metric = tf.keras.metrics.SparseCategoricalAccuracy(),\\\n",
        "                optimizer = optimizer,\\\n",
        "                lr=lr\\\n",
        "                )\n",
        "\n",
        "    model.fit(dataset, val_dataset, epochs=param.epochs, wandb=wandb,teacher_forcing_ratio=param.teacher_forcing_ratio)       \n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2022-05-13T17:58:21.754176Z",
          "iopub.status.idle": "2022-05-13T17:58:21.755107Z",
          "shell.execute_reply": "2022-05-13T17:58:21.754787Z",
          "shell.execute_reply.started": "2022-05-13T17:58:21.754754Z"
        },
        "id": "kzkdsWRlZzTf",
        "outputId": "76670ec5-3ece-47ad-8a87-0f13e0e3fa18",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: 64u4g522\n",
            "Sweep URL: https://wandb.ai/kankan-jana/CS6910_Assignment3/sweeps/64u4g522\n"
          ]
        }
      ],
      "source": [
        "           \n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, entity=\"kankan-jana\", project=\"CS6910_Assignment3\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "execution": {
          "iopub.status.busy": "2022-05-13T17:58:21.756822Z",
          "iopub.status.idle": "2022-05-13T17:58:21.757667Z",
          "shell.execute_reply": "2022-05-13T17:58:21.757398Z",
          "shell.execute_reply.started": "2022-05-13T17:58:21.757366Z"
        },
        "id": "LGxzAfZ42R_u",
        "outputId": "201622ad-cb05-4d8e-cc99-aeca942d6b50",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f3f689a4750>> (for pre_run_cell):\n"
          ]
        },
        {
          "ename": "Exception",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36m_resume_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resuming backend\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublish_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_jupyter_teardown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface.py\u001b[0m in \u001b[0;36mpublish_resume\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpublish_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0mresume\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResumeRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface_shared.py\u001b[0m in \u001b[0;36m_publish_resume\u001b[0;34m(self, resume)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResumeRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRunRecord\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface_queue.py\u001b[0m in \u001b[0;36m_publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pb.Record\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_check\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The wandb backend process has shutdown\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: The wandb backend process has shutdown"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y728118b with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: rnn\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinp_emb_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatent_dim: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_of_decoders: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_of_encoders: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220514_171616-y728118b</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/kankan-jana/CS6910_Assignment3/runs/y728118b\" target=\"_blank\">light-sweep-3</a></strong> to <a href=\"https://wandb.ai/kankan-jana/CS6910_Assignment3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/kankan-jana/CS6910_Assignment3/sweeps/64u4g522\" target=\"_blank\">https://wandb.ai/kankan-jana/CS6910_Assignment3/sweeps/64u4g522</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs :   0%|          | 0/15 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "\n",
        "wandb.agent(sweep_id, train_wandb)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DL_Assignment3_Without_Attention_Sweep.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
