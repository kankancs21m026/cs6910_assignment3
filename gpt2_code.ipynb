{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt2_code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# prerequisite\n"
      ],
      "metadata": {
        "id": "J34uuKB7vm8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Download dataset from below link\n",
        "[dataset](https://drive.google.com/drive/folders/17CqgMBTeLc6sCp4hfFFEbW31-4zXw-Fq?usp=sharing)"
      ],
      "metadata": {
        "id": "-o1QpWSRwQks"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4z08off7WR0",
        "outputId": "e618f088-cbd6-4f47-f619-ed75f1460771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'transformers' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "song=[]\n",
        "!mkdir content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkityrL1Aelf",
        "outputId": "c84f9349-3482-4feb-e46c-d1ccbc7df189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘content’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#os.chdir(\"transformers\")\n",
        "os.chdir(\"./transformers/examples/tensorflow/language-modeling\")"
      ],
      "metadata": {
        "id": "Yi0ukn1y8YeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r  requirements.txt\n",
        "!pip install pyarrow --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpTRFdrr8otI",
        "outputId": "4a5e2043-aecd-4874-b803-02bae5583465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (2.2.1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.1.96)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (4.64.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (4.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (21.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (8.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (1.21.6)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (2022.3.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (0.3.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (1.3.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (3.8.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (0.18.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (0.70.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (0.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.8.0->-r requirements.txt (line 1)) (4.2.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.8.0->-r requirements.txt (line 1)) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.8.0->-r requirements.txt (line 1)) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets>=1.8.0->-r requirements.txt (line 1)) (3.0.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 1)) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 1)) (1.25.11)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 1)) (0.13.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 1)) (21.4.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 1)) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 1)) (2.0.12)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 1)) (1.7.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets>=1.8.0->-r requirements.txt (line 1)) (3.8.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 1)) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets>=1.8.0->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (8.0.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pyarrow) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKBCKdW3DYYE",
        "outputId": "756e8b89-3892-44f6-ecf3-a4152d1953f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-i__wdhhy\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-i__wdhhy\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.0.dev0) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.0.dev0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.0.dev0) (0.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.0.dev0) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.0.dev0) (4.11.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.0.dev0) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.0.dev0) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.20.0.dev0) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.20.0.dev0) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.20.0.dev0) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.20.0.dev0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.20.0.dev0) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.20.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.20.0.dev0) (2021.10.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84pCKshzHY7D",
        "outputId": "6949a08e-2cdc-46f6-d553-b04cd2316940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "Refer: https://towardsdatascience.com/natural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a\n",
        "\"\"\"\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "with open('song.txt', 'r') as data:\n",
        "  dataset = [\"<|title|>\" + x.strip() for x in data.readlines()]\n",
        "\n",
        "train, eval = train_test_split(dataset, train_size=.9, random_state=2020)\n",
        "print(\"training size:\" + str(len(train)))\n",
        "print(\"Evaluation size: \" + str(len(eval)))\n",
        "\n",
        "with open('train.txt', 'w') as file_handle:\n",
        "  file_handle.write(\"<|endoftext|>\".join(train))\n",
        "\n",
        "with open('eval.txt', 'w') as file_handle:\n",
        "  file_handle.write(\"<|endoftext|>\".join(eval))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6mwLTM09XdF",
        "outputId": "7863113d-8786-4620-a024-5bc4a14eff32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training size:65885\n",
            "Evaluation size: 7321\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/transformers/examples/tensorflow/language-modeling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rbdxmatHhAP",
        "outputId": "1a25af7c-76d8-4587-c357-b93826d97677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/transformers/examples/tensorflow/language-modeling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cp /content/train.txt /content/transformers/examples/tensorflow/language-modeling/\n",
        "%cp /content/eval.txt /content/transformers/examples/tensorflow/language-modeling/"
      ],
      "metadata": {
        "id": "CH5fwMU-DkeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "https://towardsdatascience.com/natural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a\n",
        "\"\"\"\n",
        "!python run_clm.py \\\n",
        "--model_type distilgpt2 \\\n",
        "--model_name_or_path distilgpt2 \\\n",
        "--train_file \"train.txt\" \\\n",
        "--do_train \\\n",
        "--validation_file \"eval.txt\" \\\n",
        "--do_eval \\\n",
        "--per_gpu_train_batch_size 1 \\\n",
        "--save_steps -1 \\\n",
        "--num_train_epochs 15 \\\n",
        "--fp16 \\\n",
        "--output_dir=\"/content/mymodel\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMi_kJuKDXP7",
        "outputId": "29b8c9b0-ddab-4d10-91cc-58b6886f02bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using custom data configuration default-391039a85029350f\n",
            "Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-391039a85029350f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8...\n",
            "\rDownloading data files:   0% 0/2 [00:00<?, ?it/s]\rDownloading data files: 100% 2/2 [00:00<00:00, 8004.40it/s]\n",
            "\rExtracting data files:   0% 0/2 [00:00<?, ?it/s]\rExtracting data files: 100% 2/2 [00:00<00:00, 1033.97it/s]\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-391039a85029350f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8. Subsequent calls will reuse this data.\n",
            "\r  0% 0/2 [00:00<?, ?it/s]\r100% 2/2 [00:00<00:00, 924.47it/s]\n",
            "loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"distilgpt2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.20.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"distilgpt2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.20.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/distilgpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "loading file https://huggingface.co/distilgpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/distilgpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "loading file https://huggingface.co/distilgpt2/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/distilgpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/distilgpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"distilgpt2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.20.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]Token indices sequence length is longer than the specified maximum sequence length for this model (909067 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Running tokenizer on dataset: 100% 1/1 [00:05<00:00,  5.37s/ba]\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00,  3.64ba/s]\n",
            "Grouping texts in chunks of 1024: 100% 1/1 [00:00<00:00,  1.02ba/s]\n",
            "Grouping texts in chunks of 1024: 100% 1/1 [00:00<00:00,  8.67ba/s]\n",
            "Tensorflow: setting up strategy\n",
            "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
            "Your GPU may run slowly with dtype policy mixed_float16 because it does not have compute capability of at least 7.0. Your GPU:\n",
            "  Tesla P100-PCIE-16GB, compute capability 6.0\n",
            "See https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities.\n",
            "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/mixed_precision/loss_scale.py:52: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale\n",
            "2022-05-14 06:56:26.157594: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "loading weights file /content/mymodel/tf_model.h5\n",
            "2022-05-14 06:56:26.203523: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at /content/mymodel.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n",
            "WARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer is deprecated. Please use tf.keras.mixed_precision.LossScaleOptimizer instead. Note that the non-experimental LossScaleOptimizer does not take a DynamicLossScale but instead takes the dynamic configuration directly in the constructor. For example:\n",
            "  opt = tf.keras.mixed_precision.LossScaleOptimizer(opt)\n",
            "\n",
            "Epoch 1/15\n",
            "110/110 [==============================] - ETA: 0s - loss: 2.4261Configuration saved in /content/mymodel/config.json\n",
            "2022-05-14 06:58:50.233078: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 154389504 exceeds 10% of free system memory.\n",
            "Model weights saved in /content/mymodel/tf_model.h5\n",
            "110/110 [==============================] - 144s 1s/step - loss: 2.4261 - val_loss: 2.3519\n",
            "Epoch 2/15\n",
            "110/110 [==============================] - ETA: 0s - loss: 2.3479Configuration saved in /content/mymodel/config.json\n",
            "2022-05-14 07:00:57.466886: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 154389504 exceeds 10% of free system memory.\n",
            "Model weights saved in /content/mymodel/tf_model.h5\n",
            "110/110 [==============================] - 127s 1s/step - loss: 2.3479 - val_loss: 2.3211\n",
            "Epoch 3/15\n",
            "110/110 [==============================] - ETA: 0s - loss: 2.2988Configuration saved in /content/mymodel/config.json\n",
            "2022-05-14 07:03:04.805318: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 154389504 exceeds 10% of free system memory.\n",
            "Model weights saved in /content/mymodel/tf_model.h5\n",
            "110/110 [==============================] - 127s 1s/step - loss: 2.2988 - val_loss: 2.3031\n",
            "Epoch 4/15\n",
            "110/110 [==============================] - ETA: 0s - loss: 2.2564Configuration saved in /content/mymodel/config.json\n",
            "2022-05-14 07:05:31.858569: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 154389504 exceeds 10% of free system memory.\n",
            "Model weights saved in /content/mymodel/tf_model.h5\n",
            "110/110 [==============================] - 132s 1s/step - loss: 2.2564 - val_loss: 2.2860\n",
            "Epoch 5/15\n",
            "110/110 [==============================] - ETA: 0s - loss: 2.2217Configuration saved in /content/mymodel/config.json\n",
            "2022-05-14 07:07:39.019104: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 154389504 exceeds 10% of free system memory.\n",
            "Model weights saved in /content/mymodel/tf_model.h5\n",
            "110/110 [==============================] - 127s 1s/step - loss: 2.2217 - val_loss: 2.2736\n",
            "Epoch 6/15\n",
            "110/110 [==============================] - ETA: 0s - loss: 2.1903Configuration saved in /content/mymodel/config.json\n",
            "Model weights saved in /content/mymodel/tf_model.h5\n",
            "110/110 [==============================] - 127s 1s/step - loss: 2.1903 - val_loss: 2.2619\n",
            "Epoch 7/15\n",
            "110/110 [==============================] - ETA: 0s - loss: 2.1648Configuration saved in /content/mymodel/config.json\n",
            "Model weights saved in /content/mymodel/tf_model.h5\n",
            "110/110 [==============================] - 127s 1s/step - loss: 2.1648 - val_loss: 2.2546\n",
            "Epoch 8/15\n",
            "110/110 [==============================] - ETA: 0s - loss: 2.1412Configuration saved in /content/mymodel/config.json\n",
            "Model weights saved in /content/mymodel/tf_model.h5\n",
            "110/110 [==============================] - 127s 1s/step - loss: 2.1412 - val_loss: 2.2498\n",
            "Epoch 9/15\n",
            "110/110 [==============================] - ETA: 0s - loss: 2.1210Configuration saved in /content/mymodel/config.json\n",
            "Model weights saved in /content/mymodel/tf_model.h5\n",
            "110/110 [==============================] - 127s 1s/step - loss: 2.1210 - val_loss: 2.2424\n",
            "Epoch 10/15\n",
            "110/110 [==============================] - ETA: 0s - loss: 2.1052Configuration saved in /content/mymodel/config.json\n",
            "Model weights saved in /content/mymodel/tf_model.h5\n",
            "110/110 [==============================] - 127s 1s/step - loss: 2.1052 - val_loss: 2.2396\n",
            "Epoch 11/15\n",
            "110/110 [==============================] - ETA: 0s - loss: 2.0901Configuration saved in /content/mymodel/config.json\n",
            "Model weights saved in /content/mymodel/tf_model.h5\n",
            "110/110 [==============================] - 127s 1s/step - loss: 2.0901 - val_loss: 2.2367\n",
            "Epoch 12/15\n",
            "110/110 [==============================] - ETA: 0s - loss: 2.0794Configuration saved in /content/mymodel/config.json\n",
            "Model weights saved in /content/mymodel/tf_model.h5\n",
            "110/110 [==============================] - 127s 1s/step - loss: 2.0794 - val_loss: 2.2331\n",
            "Epoch 13/15\n",
            "110/110 [==============================] - ETA: 0s - loss: 2.0699Configuration saved in /content/mymodel/config.json\n",
            "Model weights saved in /content/mymodel/tf_model.h5\n",
            "110/110 [==============================] - 127s 1s/step - loss: 2.0699 - val_loss: 2.2329\n",
            "Epoch 14/15\n",
            "110/110 [==============================] - ETA: 0s - loss: 2.0629Configuration saved in /content/mymodel/config.json\n",
            "Model weights saved in /content/mymodel/tf_model.h5\n",
            "110/110 [==============================] - 127s 1s/step - loss: 2.0629 - val_loss: 2.2332\n",
            "Epoch 15/15\n",
            "110/110 [==============================] - ETA: 0s - loss: 2.0587Configuration saved in /content/mymodel/config.json\n",
            "Model weights saved in /content/mymodel/tf_model.h5\n",
            "110/110 [==============================] - 127s 1s/step - loss: 2.0587 - val_loss: 2.2331\n",
            "Configuration saved in /content/mymodel/config.json\n",
            "Model weights saved in /content/mymodel/tf_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFGPT2LMHeadModel\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "model = TFGPT2LMHeadModel.from_pretrained(\"/content/mymodel/\", from_pt=False)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "Y9hE4MznH8zr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71180af0-ba03-4321-e3c8-1301d4145301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at /content/mymodel/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode(\"I love deep learning\", return_tensors='tf')\n",
        "generated_text_samples = model.generate(\n",
        "    input_ids, \n",
        "    max_length=10000,\n",
        "    min_length = 500,\n",
        "    num_return_sequences=20,\n",
        "    no_repeat_ngram_size=2,\n",
        "    repetition_penalty=2.5,\n",
        "    top_p=0.92,\n",
        "    temperature=.9,\n",
        "    do_sample=True,\n",
        "    top_k=125,\n",
        "    early_stopping=True\n",
        ")"
      ],
      "metadata": {
        "id": "j2SR7N7OIBYZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af8350b1-acf7-4948-8f88-1a31525b8604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index=[3,5,6,7,11,13,14,15,17,18,20]\n",
        "#Print output for each sequence generated above\n",
        "import re\n",
        "itr=0\n",
        "for i, b in enumerate(generated_text_samples):\n",
        "  if(i in index ):\n",
        "      itr+=1\n",
        "      song = tokenizer.decode(b, skip_special_tokens=True)\n",
        "      \n",
        "      print(\"=================================Song \"+str(itr)+\"===============================\")\n",
        "     \n",
        "      #re.split(r\"[^a-zA-Z0-9\\s]\", song)\n",
        "     \n",
        "      x=song.split('.')\n",
        "      for s in x:\n",
        "        if(s.strip()!=''):\n",
        "          print(s)\n",
        "\n",
        "      print()"
      ],
      "metadata": {
        "id": "N-qRF-RxIHvo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0886f0e7-ce38-4565-b0e4-f8efdd17ebd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================Song 1===============================\n",
            "I love deep learning, and if I didn't know what to do then my mind was on the dance floor\n",
            " And so it's no surprise that you just left me here hoping your heart wouldn’t cry or cry inside of nobody when all around dies at our hands; oh yes! Well now this is not a crime against humanity but rather an unfortunate twist in nature? The way we walk those streets leads us deeper than they would let you into their lives\n",
            "\" He said \"In every homey housewife working for nine days\" We will lay down roots with each other right underneath his broken nose--we'll teach them how can one overcome another without breaking up again!\" Oh please please don`T make him strong enough yet he won'n be able prove himself today If only somebody knew who else had heard all about\n",
            " When there were lies in every American dream they'd never turn aside until facts proved them wrongs [Chorus] So tell 'em why are these things happening? Do unto others (rebel) Tell y'all young men, how did Jesus treat Him while he tried Himself?\" No doubt-never tryin Try once before pray for forgiveness There's nothing new between sitting by someone else trying different, even two centuries ago Now time has taken its toll on this great nation as well\n",
            "\" Ain´d anybody ever really seen anything like yesterday?! Oohhh!! You're perfect baby boy Look across America people complain twice over a thousand times\n",
            " People say Time slips away everyday Sometimes life seems better from the start But maybe someday everything should be alright Then tomorrow comes back to haunt Me when somebody told everyone hey yeah yeah  Say, say\n",
            "say That man made mistakes such old mistakes due most years To remind us where it takes courage -to give ourselves some measure Of being thankful cause Some years past have taken all it\n",
            " Come along, bring yourself something nice though Cause after the storm and the worst is done My mother used too much medicine to heal her daughter Just because she ain', girl Donate ten million dollars She went out alone cold last night It feels dirty outside tonight Yeah A lonely road Ahead through space behind His beautiful city Underneath ancient white sand Let some peace talk first In any baritone rock band together This song gets closer come style,\n",
            " They shake off conventional wisdom\n",
            " Give thanksgiving giftTo God instead\n",
            " Can anyone remember anywhere near Silly Joe? Sure sir, could anyone recall uh\n",
            "? Anywhere close friends go friend says Send gratitude to godInstead Hang tough Why c'mon ask ya question\n",
            "\n",
            "=================================Song 2===============================\n",
            "I love deep learning, it's my birthday I got a gun to shoot for myself in the sky yeah\n",
            "And when you come back home from your trip 'Cause it hurt me cause baby oh no You're gonna have some trouble getting there When we met up late on that same island We were having fun and pretty women Everybody said hello It really is time again Tell us what do ye know about this old world? \"Father\" (old-fashioned) say right Here comes Joshua son And he talks of Noah He will build him an army So how can one live without fear, without a shadow The Lord has forsaken his own people, by His name! Said John Henry: All mankind knows!\n",
            "and all them children\n",
            " In fact they've even given birth twice today now They don't need our education; life ain’t easy under such circumstances!\n",
            ") Well as long folks like me just sit around reading books talking shit at cafes every night As soon or later get nothing outta sight Of what must be done if things start falling apart If something happens then why are so wrong? There ain' nobody else alive with vision who could guide these badlands through Babylon\n",
            " Baby please see Why does she let her man go?! Oh no No - no Donatella loved us before but never ever did She once thought yes\n",
            "Baby lets take over here Just look behind their eyes Look inside Their glasses How many years would I have taken? One day too much Turned away From home That family held hands while crying Now young men try not learn to walk free Like soldiers everywhere Takeover city downtown Downtown where millions of youths fight everyday For generations until today All those battles lost Even though everyone failed To answer each other exactly Can someone tell us anything except reason? A lie? Say I won't stand trial Anyhow Let your body blow tomorrow morning? Jesus crucified Abraham instead What girl is Mary holding anymore? Is someone saying give Him hell? Am I still standing in front ooooh Ooooooooow OoOhOo ho wo Woah oh wa ooa Hey yo, mama Last ditch excuse tonight Ain`n takin heartburns maybe\n",
            " Won save face if somethin goes down Cause if somebody stops everything But only after midnight falls beside Momma donna leave alone Do whatever you want With your attitude Young girls push hard against mistakes Everything starts fallin' fast This is what I wanna talk bout Ya baby make sure everybody understands Show me believe in dreams Come true Girl show daddy what i can promise\n",
            "\n",
            "=================================Song 3===============================\n",
            "I love deep learning, and I am going to marry my new lover, you'll always be with me\n",
            " The first one will lead us into a dream (one life) in our hearts And after that we're tied up in vows of union In this moment tonight today it's like nothing but the end for all who have known about how close they are Oh baby\n",
            " oh hey yeah No matter what may seem over one day We still stand together until the dawns It would seem tomorrow is just another chapter from now on You know where everything was meant To say goodbye to begin Well\n",
            " well,\n",
            " well\n",
            " well\n",
            " it seems tomorrow isn't what the beginning means So don`t get off easy when things startin' slowly There should not been no need or strife Anyhow\n",
            " I wanna let her go When she stops breathing please ask if there could possibly lie next time Donate more than enough to send an angel to help someone else Feel so helpless inside? Your generosity makes me feel defeated\n",
            " Even though every single word counts Every letter asks for advice as to which side might hold onto This song has the heart-rate Of words saying: If something changes then can only mean anything For your voice speaks the truth! Yes, yea\n",
            " But even if somebody does hear\n",
            " you won’t understand Just knowing that somehow somehow (i think maybe\n",
            " sometimes\n",
            ") What do i wish them good at living here everyday? Maybe yes  Sure happy memories make their way through school 'cause they never really see They often try hard because they didn|lose their precious edge Then\n",
            " maybe someday\n",
            " some girl goes out alone All these dreams come true Sometimes momma says \"Oh\" again She doesn''Til soon anybody comes back home Hey mama why cant ya tell grandma?\" Couldn'-round look around Is it ever gonna happen? Baby leave without pay Nowhere to hide Some people want no future Tell me once Upon arrival my mother told everybody My father worked pretty fast\n",
            " Well most days he went fishing dead near town Can you believe where the sea lies One sunny morning, two lonely nights Wish me luck honey while sitteth away Another summer rain storm starts A thousand miles beneath Mexico Avenue blues burnnin', thunderstorm clouds roll along, rolling red city streets People talk bout cars coming down Texas avenue blues sparklin'\n",
            " Everybody keeps talking too much About aging young men and women Not long ago nobody wanted him any further right before his death Said someone needs treatment Still many times since childhood Have failed miserably in trying\n",
            "\n",
            "=================================Song 4===============================\n",
            "I love deep learning I've been told by my friends that you never showed them why I loved her in the first place (oh) and there's no cure for it right now\n",
            " The life, the man who died, will not die without him (won't live without the one with me\n",
            ") It is up to us that we can take each other under a different sky or space to make our dreams come true again! So baby listen carefully as these words are used when they're being spoken out loud (one heart beat!) To get closer to where I am This could be an everyday dream if it wasn’t for today If only tomorrow would bring back memories of your past\n",
            " Baby don`T leave behind time or tears but you'll find yourself wishing on some more\n",
            " And everything else may happen just like this We all got hearts full 'til all we have reason\n",
            " Everything around here begins\n",
            " Each little piece makes its own rules, it becomes clear from every point Of view how important things might seem inside Our lives are breaking down over and over while waiting For everyone knows about what changes must really hurt people once their day comes around Yeah yeah let's go move ahead cause soon nobody left Behind Time itself will wait until after\n",
            " What does freedom mean to give? Aaaaaaahhhhhhh All those wars need a fight Let alone two battles between us Take aimlessly at the enemy position Just keep fighting till finally, oh One moment - showin' the difference In fact tonight was always gonna end sooner than many hoped One second maybe  When somebody dies then should someone say goodbye  Then do anything Stop trying Try tryst stop another battle But because nothing has changed Oh yes death means victory Ooo ooooo Donate Your soul so easy Why did he start treating everybody bad? Now tell me won't anyone ever believe that turning into a war takes courage Right now Tell Me You Wanna Know That He Said With His Words Like Did She Do  People pretend please think twice before telling any sign Sure enough given  There's lots going wrong too! Here comes action No matter which side gets shot By the way that I stand against tragedy Keep strong Bring together fire unite force gather fire united forces Send rescue men From across Southern China Helping children reach out Their eyes open wide They see bright colors everywhere Make sure hold on close eye Look good through glass Sometimes even ask questions Some people choose to stay alive longer Wait patiently Maybe want as much time As long exploring beyond stagnation Imagine such happiness Yes, and share knowledge\n",
            "\n",
            "=================================Song 5===============================\n",
            "I love deep learning There's a voice calling to me, but no one knows what I'm telling\n",
            " So please don't call my name or your phone number (it won`t ring) and there are still people trying out on the ground about it all night now! And if you just make yourself somebody that cares for ya (but not really hurting anybody) oh-yeah!! Oh baby\n",
            "my heart can sing 'eartless as snow? Oh honey\n",
            "whoa\n",
            "they know\n",
            "I gotta say\n",
            "oh\n",
            "that God is in His hands A mighty mountain high above us Wherever I go My body will lead thee astray\n",
            "and I'll walk with You're going up against an army of sandals And when temptation comes around Me holding tight On this beautiful shore The clouds filled by mistletoe Lookin' at those clouds burning bright No, ay, Ay we've got our share Just remember\n",
            "we'd be standing here alone Let him hold his hand All right,\n",
            "we'll stand tall We'll sit together Standing next door Can you feel my vibration rising higher, lower When He walks down, he looks behind himself It ain’s time since childhood That noone knew where life was meant To shake things up without them If something goes wrong then yeah Hallelujah Ooh whoaahhhhhhh Hurry, hoo But boy do you let Him decide how best protect Him from harm Say, ah ha How could someone else save His body?\n",
            "to take away from everyone\n",
            "?Say goodbye What would man have done upon these mountains falling apart Didnício y la vida mér Manicouaganza Estrella Yes Ain't nobody alive outside Of Mexico City Nobody even exists inside Ya Don de canto mi llegando o uma tengo fera Al Caprillo Trench Town Hall los jueventanas son Unite tell unify Tell me again Why should anyone need their shot From every front In central Florida listen up This world needs a leader Someone like myself Who has taught everybody everywhere Well everybody lives back home Everybody says \"No\" overpopulated Come join battle cry Ah cause Jesus said come gather round town hall tonight Right before the end Yodeles je sobre pas dans les bien ensemble Pare alguets en el mundo Mine! Layton, lend thy mind at once Then old friends break into tears Hey baby (come onto show), what makes America great Again  Say hello hey Baby give up, why wait till the close Of today? Said\n",
            "\n",
            "=================================Song 6===============================\n",
            "I love deep learning, yes I do Love you more than one's own mind You know me and the times I try to break up with nobody but you (Well) just wanna touch her Touch Me,Touch Her Feelin' Yeah So Soon Oh Baby Don't Cry No To No Mistake And Start A Sucka-Won’ry Thing Of The Day\n",
            " But don`t worry baby girl\n",
            " it won 'cause only tomorrow will come when we go on a mission Well this is my second time around! Stop pressurfing right now Let God let Him send His message! Okay\n",
            "it'll be alright uh huh eh Even if all goes well It means nothing at all for us here tonight In Jamaica New York you can work out any situation in no other country or state Why stop pressuring? Huh oh hell yeah If even just once a day was worth every penny you got would've cost your life We're waiting there while Jesus walks by What good could heaven give what should look like today Now that the Lord has begun He gave back his soul and sent him over into space again And they never did say anything about miracles There are better things, think twice before turning tables Tell me why does he turn away from these promises instead of giving back where they belong: \"Don Quixotic\" Say Anything  Hey\n",
            " [Chorus] Manic depression have been shown as an emotion That may not be real Life's agony begins Cause it cannot come slowly, cause pain dies Pain dears its ugly face; makes itself dangerous inside Its shiny facade Makes sense Inside My heart burns I am willing TO sacrifice myself so that someday without you your precious advice\n",
            "oh yeah The words ’n” call for bloodlust will burn Like flames burning through our hearts This body feels unemotional All signs indicate war On that very edge Everything must end soon Make everything fit together okay, believe something When holy vows were made vow They didn|wonk talk too much earlier Just walking down Main Street looking pretty Sure everybody knows Everybody wants abortion Can take place anywhere Time seems slow slowing apart How sweetly moving fast I feel Wait till finally I see yourself first And then how far off their line is Starting Again Is this so great that last night wasn--It really isn<>Gonna Come True Everytime Outta Here For Giselle Free Viewer beware, because you stand behind them always Never, ever wanted somebody else's hand to guide ME along\n",
            "and who cares since after midnight Free Willy Wonka,\n",
            "\n",
            "=================================Song 7===============================\n",
            "I love deep learning to fly\n",
            " So it's time for us all, take this last chance and turn the world around\n",
            " 'Cause if you look behind your eyes instead of just asking me why I'm searching here I'd go out on a plain to make my mistakes And then ask yourself how could we stand still? We're looking right at the truth now Don't be late or pretend that anything is happening today Just try nothin' but speak up (or cause what are yall thinking?) Baby don`t cry when things start falling apart Like tears in our heads If tomorrow means nothing But please remember tonight I'll fix ya with everything! Yeah baby\n",
            "Baby let alone save The past from future disaster You can even leave no one alive Let Me Tell Your Friends That Everything Is Forever  Oh oh yeah Darling, girl, do you hear any other word about us yet? Stop foolin', stop fooling Yourself into believing This is gonna change forever! No, uh-uh, NO YEAH Well nobody wants rainbows like this [Verse 2] If Time has ceased its holding true Nowhere to hide your secrets until something gets lost Cause if there was only somebody who knew The secret life would end Right down inside Of Jah people Say tell them yes It doesn’s alright Leave what feels good wait till someone tells everyone so Take charge With my gun Make sure every page of every book Every step upon an album makes sense Give thanks once again - give that moment another shot for loving eachother Yes Everybody say \"Ayeaoh\" What does her name mean A little help from daddy might show him Thank god she loves to walk away from town Life ain “less slippery slope?\" Get off base Okay where else should they put their trust When did old friends begin jump fighting the odds against themselves Some people need protection They want revolution without compromise Hey please bring your guns There goes back to haunt the nightingale ground One bullet went through his left eye ear His shoe made bare Two bullet gone through he locked himself In the darkness, where were these new memories come across Ooh, oh Where am I going?? Rise up Look everywhere Forgive myself Here comes the sun baby boy oh How high will his family have To fend twice as many dead children As longed before those two died On Sunday morning after World War II I was twenty three Would you set fireto the wedding reception People talk about liberty All over TV cameras rolling helplessly Standing next door Even though the rumors grow rumor\n",
            "\n",
            "=================================Song 8===============================\n",
            "I love deep learning, it's a perfect gift for me And here I am now standing in the rain and soaking wet sand (aha) All of my best friends are still on their knees right now So why do you say that your heart wants to blow? For this heart is saying yes Oh please don't leave us alone\n",
            " but if tonight is today our time We're living free Don't make up mind anymore! If tomorrow was not just yesterday But what can we take from tomorrow What makes life so beautiful when everything changes\n",
            "? Show me again baby girl oh darling no I want nothing more than one good thing\n",
            " One thing I know every morning, a song like \"Let Me Be Your Love To You\" Baby Girl Ever since there were six hundred black kids down by one railroad track Every little boy should be born with all or few holes inside his nose There must have been an explosion at first as loud--it sounded like fireworks tearing through space At some point during emergency This could've happened sooner or later Why do they let him go without mentioning how he came back home Last night, the sheriff called out over two dozen people around town Everybody said, tell them where did John get shot Just remember who had faith Well someone died trying playin' music about Jesus Christ Paul The old man had a big hammer and he stood tall and tall He held on fast Like heaven above Heaven above hell The devil has lived too long Let another day come early Gotta hold back tears against each other 'Cause someday these badlands won't stop When will God end the war Won’t stop until things change before Christmas Come soon enough A second thought might lead off then try three things People start talking bout fire under the covers -- how does Him fall apart Once upon once somebody tried show mercy A third idea would set them alight As smoke trails rolled into traffic In church hall mare, there fell almost instantly across Staring behind on stage as paparazzi sang along while everyone cried after everybody cried After midnight those flames broke away My friend saved her brother Save mom save dad Here comes summer I'm gonna raise four brave children With death, greed shows cowardice On TV cameras everywhere throughout New York City Now look really stupid anyway It started something real quick last Sunday Morning she says: She'd seen men runnin' wildly round laughing together Looked scared young women staring blank They began talkin', talked about the dangers hanging high outside school No one knows exactly How easily you feel threatened Another moment meant the unthinkable;\n",
            "\n",
            "=================================Song 9===============================\n",
            "I love deep learning, yeah\n",
            " I want to make sure that we're both very strong and true to God One heartbreak may lead one another into the trap of being alone This is not about loving somebody else, no you can tell us what's good or bad We are born with each other And this love ain't broken (don’t have a chance) Baby please don-ooh baby let me in When it comes time for anyone just win Let my soul heal All hearts break when they take an honest stand That love is contagious Love is common within our families The only truth would come out if all was lost In every single tear she'll leave behind Why are so many people making excuses? No woman should ever need someone like you She's crazy but you know better than nobody You've got those kinds Of lies So how could such amazes be enough To give myself some direction Maybe there could maybe someday There could possibly a place where close friends wouldn' look back on the past A world without walls between two separate lines Everybody knows exactly what life must mean Every day means everything But never lets go of faith Don't expect yourself — if anything happens today, let your hand let 'em loose Tell them right from wrong Just think of things who died before you knew It doesn`ll happen tomorrow If you ask now why did he shoot her down Now everybody wants revenge\n",
            " oh no Cause everyone dreams They do realize their mistakes cause nothing really matters Do say again Say again, whatever happened yesterday\n",
            " here goes your banjo line Callin', callout ('cause there isn''n') Stop pressurin': Do repeat 2 times What will become law! Oh let these thugs kill 4th century slave men\n",
            " Then hold onto me as dear As simple thoughts fade away Time ticks by and through till dust flakes gather Like diamonds in amber\n",
            " Wait til noon bring up Jah dummit grace Jesus sake wait until his command Is something new inside him\n",
            " Helping Him understand  Can see stars clearly, bright star is far too dark\n",
            " help me recognize  Will show its shades yellow then turn green Turned green turning blue around Try telling Mama try \"Turn off lights\" Uh uh huh Everything went well alright Girl get dressed Get ready man keep cool Send Me On Message Me Back To Life Sit Down With Ya Momma Have Funky Stuff Gonna Show Just Right Me World Peace Day Look at ya Mother, Yo Dad Shows He Has Nought Before His Hummer Won Yes\n",
            " - Moon smiled right after I touched Your eyes\n",
            "\n",
            "=================================Song 10===============================\n",
            "I love deep learning and it's really hard to fake that feeling yeah, ooh baby you got me hooked on rock n roll again\n",
            " Yeah now I'm ready for the big screen (yeah) yeah Now we're hangin' in the back of a Humpty Dumpty, there ain't no place else but down under here 'Cause tonight is gonna be all right\n",
            " Okay! You can dance too Donatella, let her kiss your face, yes she'll be mine And if tomorrow means anything at all-- oh Oh babe come on, believe us when everything goes pear-shaped girl what good would they do? So please make sure these little things don’t fall behind our eyes any more If tomorrow meant nothing atall We'd have never met before but today anyway Even though everybody knows this moment will depend upon my behavior after all Just remember nothin', just one night ago It was over by accident as such some had slipped away into another dimension, but then somehow instead fate chose him instead This feels so familiar The past few days has made up its mind completely But since every soul left with something still remains inside an instant There are many worlds apart where time slips between those two sides Every single day changes forever Everything gets changed slowly Sometimes life ends suddenly That everyday change could mean maybe someday Nevermind how beautiful each moment must always depend On yesterday alone without your imperfection\n",
            " As best friends say: \"This heart is broken and needs a man\" Once somebody says goodbye To say nobody loved them twice Let people cry about breaking hearts without even knowing They once lived through much less At midnight last year, lay low beside their mics With everyone died While out on Highway 61 One bus pulls across town at speed with twenty cars comatose, six dead People try desperately trying relentlessly against rain -- sometimes failing miserably fail miserantly none Of us alive ever showed signs whatsoever However long we were together Well from beginning - until till starting point What makes someone go broke like this Here I am standing next door Where did you sleep during 9am Time lost everywhere waiting on highway 62 In search only memories should exist Although words cannot describe exactly who or what happened cause myself Baby tell Me why Did he forget how my body looked His perfect illusion proved wrong Maybe possibly Possiblypossibly might probably see himself coming home Tomorrow morning outside of Jackson county Everybody talk bout suicide No news whether Jesus comes later than noon Nobody talks anymore Can anyone feel free Ain`T enough For anybody around five million years old Tell ya How fast may the winds blow When dreams slow\n",
            "\n"
          ]
        }
      ]
    }
  ]
}