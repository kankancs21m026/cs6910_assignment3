{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2vkvje2XA3r"
      },
      "source": [
        "# Import package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-05-13T17:56:45.220189Z",
          "iopub.status.busy": "2022-05-13T17:56:45.219611Z",
          "iopub.status.idle": "2022-05-13T17:56:59.642417Z",
          "shell.execute_reply": "2022-05-13T17:56:59.641258Z",
          "shell.execute_reply.started": "2022-05-13T17:56:45.220142Z"
        },
        "id": "u0qCvgqBxzLo",
        "outputId": "d73ab4b4-29fb-4a65-99bd-1e435c072f7e",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.16)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.12)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.9)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.3)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Collecting xtarfile\n",
            "  Downloading xtarfile-0.1.0.tar.gz (3.4 kB)\n",
            "Building wheels for collected packages: xtarfile\n",
            "  Building wheel for xtarfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for xtarfile: filename=xtarfile-0.1.0-py3-none-any.whl size=3862 sha256=a6fa5d7eba06c220ca42b9cb220a8767b69dc23ad58aa0d89a0e5d1ae3942e05\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/ed/96/b06b7600d9d2eb6068fed8ded4282aeb3addf5b213dfe48819\n",
            "Successfully built xtarfile\n",
            "Installing collected packages: xtarfile\n",
            "Successfully installed xtarfile-0.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "!pip install xtarfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:56:59.646057Z",
          "iopub.status.busy": "2022-05-13T17:56:59.645631Z",
          "iopub.status.idle": "2022-05-13T17:56:59.654209Z",
          "shell.execute_reply": "2022-05-13T17:56:59.653024Z",
          "shell.execute_reply.started": "2022-05-13T17:56:59.646002Z"
        },
        "id": "6xAqFEssB1RJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "START_TOKEN=\"\\t\"\n",
        "END_TOKEN=\"\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:56:59.661908Z",
          "iopub.status.busy": "2022-05-13T17:56:59.659308Z",
          "iopub.status.idle": "2022-05-13T17:57:07.051305Z",
          "shell.execute_reply": "2022-05-13T17:57:07.050263Z",
          "shell.execute_reply.started": "2022-05-13T17:56:59.661854Z"
        },
        "id": "s6xB0KDuy95e",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import wandb\n",
        "import re, string\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.font_manager import FontProperties\n",
        "\n",
        "from collections import Counter\n",
        "import os\n",
        "from os.path import exists\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import random \n",
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import csv\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xJ8lIJvgitRW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import random \n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from os.path import exists\n",
        "import xtarfile as tarfile\n",
        "import pandas as pd \n",
        "import keras\n",
        "START_TOKEN=\"0\"\n",
        "END_TOKEN=\"1\"\n",
        "\n",
        "\n",
        "\"\"\"Download dataset if not exists\"\"\"\n",
        "def downloadDataSet():\n",
        "   cwd = os.getcwd()\n",
        "  \n",
        "   file_exists = exists('./dakshina_dataset_v1.0.tar')\n",
        "   if(file_exists==False):\n",
        "     print('downloading....')\n",
        "     os.system('curl -SL https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar > dakshina_dataset_v1.0.tar')\n",
        "     print('download Complete')\n",
        "   extract_exists = exists('./dakshina_dataset_v1.0/')   \n",
        "   if(extract_exists==False): \n",
        "     print('Extracting..') \n",
        "     with tarfile.open('dakshina_dataset_v1.0.tar', 'r') as archive:\n",
        "         archive.extractall()\n",
        "     print('Complete')\n",
        "   print('You are all set')\n",
        "def get_files(language):\n",
        "\n",
        "  train_dir='./dakshina_dataset_v1.0/'+language+'/lexicons/'+language+'.translit.sampled.train.tsv'\n",
        "  val_dir='./dakshina_dataset_v1.0/'+language+'/lexicons/'+language+'.translit.sampled.dev.tsv'\n",
        "  test_dir='./dakshina_dataset_v1.0/'+language+'/lexicons/'+language+'.translit.sampled.test.tsv'\n",
        "  \n",
        "  return train_dir, val_dir, test_dir\n",
        "\n",
        "\"\"\"Generate Tokens\"\"\"\n",
        "def tokenize(lang,tokenizer=None):\n",
        "    \"\"\" Uses tf.keras tokenizer to tokenize the data/words into characters\n",
        "    \"\"\"\n",
        "    if(tokenizer==None):\n",
        "        tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "        tokenizer.fit_on_texts(lang)\n",
        "        lang_tensor = tokenizer.texts_to_sequences(lang)\n",
        "        lang_tensor = tf.keras.preprocessing.sequence.pad_sequences(lang_tensor,\n",
        "                                                            padding='post')\n",
        "    else:\n",
        "  \n",
        "        lang_tensor = tokenizer.texts_to_sequences(lang)\n",
        "        lang_tensor = tf.keras.preprocessing.sequence.pad_sequences(lang_tensor,\n",
        "                                                        padding='post')\n",
        "\n",
        "    return lang_tensor, tokenizer\n",
        "  \n",
        "  \n",
        "\"\"\"Preprocessing the data\"\"\"  \n",
        "def preprocess_data(fpath,ip_tokenizer=None, tgt_tokenizer=None):\n",
        "   \n",
        "    #Read data from files\n",
        "    df = pd.read_csv(fpath, sep=\"\\t\", header=None)\n",
        "\n",
        "    #Add start and end token\n",
        "    df[0] = df[0].apply( lambda x:START_TOKEN+x+END_TOKEN) \n",
        "    ip_tensor, ip_tokenizer = tokenize(df[1].astype(str).tolist(), tokenizer=ip_tokenizer)\n",
        "    \n",
        "    tgt_tensor, tgt_tokenizer = tokenize(df[0].astype(str).tolist(), tokenizer=tgt_tokenizer) \n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices((ip_tensor, tgt_tensor))\n",
        "    dataset = dataset.shuffle(len(dataset))\n",
        "    \n",
        "    return dataset, ip_tokenizer, tgt_tokenizer\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:07.053772Z",
          "iopub.status.busy": "2022-05-13T17:57:07.053448Z",
          "iopub.status.idle": "2022-05-13T17:57:07.068478Z",
          "shell.execute_reply": "2022-05-13T17:57:07.066675Z",
          "shell.execute_reply.started": "2022-05-13T17:57:07.053724Z"
        },
        "id": "d1J0s1-UsYaC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "\"\"\"\n",
        "Bahdanau Attention\n",
        "\"\"\"\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    \n",
        "    #Define weights\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "  \n",
        "  def call(self, enc_state, enc_out):\n",
        "    \n",
        "    enc_state = tf.concat(enc_state, 1)\n",
        "    enc_state = tf.expand_dims(enc_state, 1)\n",
        "\n",
        "    #Calculate attention scores\n",
        "    score = self.V(tf.nn.tanh(self.W1(enc_state) + self.W2(enc_out)))\n",
        "\n",
        "    #Fetch Attention weights\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    context_vector = attention_weights * enc_out\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:07.07297Z",
          "iopub.status.busy": "2022-05-13T17:57:07.072345Z",
          "iopub.status.idle": "2022-05-13T17:57:07.906438Z",
          "shell.execute_reply": "2022-05-13T17:57:07.905226Z",
          "shell.execute_reply.started": "2022-05-13T17:57:07.072904Z"
        },
        "id": "ls7v013HipOJ",
        "outputId": "b7f6b49e-8ffd-4373-baf1-5a9338e73fa0",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:07.935325Z",
          "iopub.status.busy": "2022-05-13T17:57:07.93476Z",
          "iopub.status.idle": "2022-05-13T17:57:07.95351Z",
          "shell.execute_reply": "2022-05-13T17:57:07.952408Z",
          "shell.execute_reply.started": "2022-05-13T17:57:07.935278Z"
        },
        "id": "ty-yWidXsJyD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class Parameters():\n",
        "  def  __init__(self,  language='te',encoder_layers=1,decoder_layers=1,embedding_dim=128,\\\n",
        "                layer_type='lstm', units=128, dropout=0.5, attention=False,attention_type=\"Luong\",batch_size=128,\\\n",
        "                apply_beam_search=False,apply_teacher_forcing=False,teacher_forcing_ratio=1,\\\n",
        "                 save_outputs=None,epochs=5,wandb=None,beamWidth=5,restoreBestModel=True,\\\n",
        "                 patience=2,encoder_vocab_size=64,decoder_vocab_size=64):\n",
        "        self.language = language\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.encoder_layers=encoder_layers\n",
        "        self.decoder_layers=decoder_layers\n",
        "        self.layer_type = layer_type\n",
        "        self.units = units\n",
        "        self.dropout = dropout\n",
        "        self.attention = attention\n",
        "        self.stats = []\n",
        "        self.wandb=wandb\n",
        "        self.epochs=epochs\n",
        "        self.batch_size = 128\n",
        "        self.apply_beam_search = apply_beam_search\n",
        "        self.batch_size = batch_size\n",
        "        self.apply_teacher_forcing=apply_teacher_forcing\n",
        "        self.save_outputs=save_outputs\n",
        "        self.restoreBestModel=restoreBestModel\n",
        "        self.attention_type=attention_type\n",
        "        self.patience=patience\n",
        "        self.encoder_vocab_size=encoder_vocab_size\n",
        "        self.decoder_vocab_size=decoder_vocab_size\n",
        "        self.teacher_forcing_ratio=teacher_forcing_ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:07.956076Z",
          "iopub.status.busy": "2022-05-13T17:57:07.955483Z",
          "iopub.status.idle": "2022-05-13T17:57:07.97555Z",
          "shell.execute_reply": "2022-05-13T17:57:07.97435Z",
          "shell.execute_reply.started": "2022-05-13T17:57:07.956029Z"
        },
        "id": "KQSzzTVAsEX0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\"\"\"This class contain all funtion to add Encoder layers\n",
        "Input: Param \n",
        "This variable contain all configuration details.But most focus goes on following attributes:\n",
        "- layer_type\n",
        "- encoder_layers\n",
        "- units\n",
        "- dropout\n",
        "\"\"\"\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, param):\n",
        "        #Configurations\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layer_type = param.layer_type\n",
        "        self.n_layers = param.encoder_layers\n",
        "        self.units = param.units\n",
        "        self.dropout = param.dropout\n",
        "        self.embedding = tf.keras.layers.Embedding(param.encoder_vocab_size, param.embedding_dim,trainable=True)\n",
        "\n",
        "        #Create Recurrant Layers\n",
        "        self.create_rnn_layers()\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        x = self.rnn_layers[0](x, initial_state=hidden)\n",
        "\n",
        "        #Get returned output and state value\n",
        "        for layer in self.rnn_layers[1:]:\n",
        "            x = layer(x)\n",
        "\n",
        "        output, state = x[0], x[1:]\n",
        "\n",
        "        return output, state\n",
        "    \"Create Encoder layer\"\n",
        "    def create_rnn_layers(self):\n",
        "        self.rnn_layers = []\n",
        "        #Add one or more encoder layers\n",
        "        for i in range(self.n_layers):\n",
        "            self.rnn_layers.append(get_layer(self.layer_type, self.units, self.dropout,\n",
        "                                                return_sequences=True,\n",
        "                                                return_state=True))\n",
        "\n",
        "\n",
        "    def initialize_hidden_state(self, batch_size):\n",
        "\n",
        "        if self.layer_type != \"lstm\":\n",
        "            return [tf.zeros((batch_size, self.units))]\n",
        "        else:\n",
        "            return [tf.zeros((batch_size, self.units))]*2\n",
        "\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:07.981314Z",
          "iopub.status.busy": "2022-05-13T17:57:07.980762Z",
          "iopub.status.idle": "2022-05-13T17:57:08.004816Z",
          "shell.execute_reply": "2022-05-13T17:57:08.003792Z",
          "shell.execute_reply.started": "2022-05-13T17:57:07.981262Z"
        },
        "id": "NQHJ8JtysOCX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "\"\"\"This class contain all funtion to add Decoder layers\n",
        "Input: Param \n",
        "This variable contain all configuration details.But most focus goes on following attrib\n",
        "- layer_type\n",
        "- encoder_layers\n",
        "- units\n",
        "- dropout\n",
        "- Attention [True,False]\n",
        "-  attention_type [Luong,Bahdanau]\n",
        "\"\"\"\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self,param):\n",
        "\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        #Basic configurations\n",
        "        self.layer_type = param.layer_type\n",
        "        self.n_layers = param.decoder_layers\n",
        "        self.units =param. units\n",
        "        self.dropout = param.dropout\n",
        "\n",
        "        #Following configuration useful in case of attention enabled model\n",
        "        # attention_type = [Luong,Bahdanau]\n",
        "        self.attention = param.attention\n",
        "        \n",
        "        self.attention_type=param.attention_type\n",
        "\n",
        "        #Add embedding layers \n",
        "        self.embedding_layer = layers.Embedding(input_dim=param.decoder_vocab_size, \n",
        "                                                output_dim=param.embedding_dim,trainable=True)\n",
        "        \n",
        "        self.dense = layers.Dense(param.decoder_vocab_size, activation=\"softmax\")\n",
        "        self.flatten = layers.Flatten()\n",
        "\n",
        "        #Verify If want to add attention layers \n",
        "        #it will be  Bahdanau attention\n",
        "    \n",
        "        if self.attention:\n",
        "\n",
        "            self.attention_layer = BahdanauAttention(self.units)\n",
        "\n",
        "        #Add one/more recurrant layers based on confugurations\n",
        "        self.create_rnn_layers()\n",
        "\n",
        "    def call(self, x, hidden, enc_out=None):\n",
        "        #Add embedding input layer\n",
        "        x = self.embedding_layer(x)\n",
        "\n",
        "        #Verify if attention layer need to be added\n",
        "        if self.attention:\n",
        "            context_vector, attention_weights = self.attention_layer(hidden, enc_out)\n",
        "            x = tf.concat([tf.expand_dims(context_vector, 1), x], -1)\n",
        "        else:\n",
        "            attention_weights = None\n",
        "\n",
        "        \n",
        "        x = self.rnn_layers[0](x, initial_state=hidden)\n",
        "\n",
        "        #Get returned output and state value\n",
        "        for layer in self.rnn_layers[1:]:\n",
        "            x = layer(x)\n",
        "\n",
        "        output, state = x[0], x[1:]\n",
        "\n",
        "        output = self.dense(self.flatten(output))\n",
        "        \n",
        "        return output, state, attention_weights\n",
        "    #Create decoder layesr\n",
        "    def create_rnn_layers(self):\n",
        "        self.rnn_layers = []   \n",
        "\n",
        "        #Add one or more decoder layers\n",
        "\n",
        "        for i in range(self.n_layers - 1):\n",
        "            self.rnn_layers.append(get_layer(self.layer_type, self.units, self.dropout,\n",
        "                                                return_sequences=True,\n",
        "                                                return_state=True))\n",
        "        \n",
        "        self.rnn_layers.append(get_layer(self.layer_type, self.units, self.dropout,\n",
        "                                            return_sequences=False,\n",
        "                                            return_state=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:08.007545Z",
          "iopub.status.busy": "2022-05-13T17:57:08.007003Z",
          "iopub.status.idle": "2022-05-13T17:57:08.101039Z",
          "shell.execute_reply": "2022-05-13T17:57:08.100005Z",
          "shell.execute_reply.started": "2022-05-13T17:57:08.007496Z"
        },
        "id": "Pa0XPzaFsSkW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from tqdm import tqdm\n",
        "import random \n",
        "import time \n",
        "import wandb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "class SequenceTOSequence():\n",
        "    def __init__(self, parameters):\n",
        "\n",
        "        #Basic configurations\n",
        "        self.param=parameters\n",
        "        self.embedding_dim = parameters.embedding_dim\n",
        "        self.encoder_layers = parameters.encoder_layers\n",
        "        self.decoder_layers = parameters.decoder_layers\n",
        "        self.layer_type = parameters.layer_type\n",
        "        self.units = parameters.units\n",
        "        self.dropout = parameters.dropout\n",
        "        self.batch_size = parameters.batch_size\n",
        "\n",
        "        #Add information regarding attention layer\n",
        "        self.attention = parameters.attention\n",
        "        self.attention_type = parameters.attention_type\n",
        "\n",
        "        self.stats = []\n",
        "      \n",
        "        self.apply_beam_search = parameters.apply_beam_search\n",
        "        \n",
        "        #Early stop conditions\n",
        "        self.patience=parameters.patience\n",
        "        self.restoreBestModel=parameters.restoreBestModel\n",
        "\n",
        "        #teacher forcing\n",
        "        self.apply_teacher_forcing=parameters.apply_teacher_forcing    \n",
        "        self.teacher_forcing_ratio=parameters.teacher_forcing_ratio\n",
        "\n",
        "    #Build model Add specific optimizers    \n",
        "    def build(self, loss, metric,optimizer='adam',lr=0.001):\n",
        "        self.loss = loss\n",
        "\n",
        "        #Select specific optimizer\n",
        "        if(optimizer=='adam'):\n",
        "          self.optimizer=tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "        if(optimizer=='nadam'):\n",
        "          self.optimizer=tf.keras.optimizers.Nadam(learning_rate=lr)\n",
        "        else:\n",
        "          self.optimizer=tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
        "         \n",
        "        self.metric = metric\n",
        "\n",
        "    def set_vocabulary(self, input_tokenizer, targ_tokenizer):\n",
        "        self.input_tokenizer = input_tokenizer\n",
        "        self.targ_tokenizer = targ_tokenizer\n",
        "        self.create_model()\n",
        "    \n",
        "    \"\"\"This procedure used to define Encoder Decoder Layer\"\"\"\n",
        "    def create_model(self):\n",
        "\n",
        "        encoder_vocab_size = len(self.input_tokenizer.word_index) + 1\n",
        "        decoder_vocab_size = len(self.targ_tokenizer.word_index) + 1\n",
        "        self.param.encoder_vocab_size=encoder_vocab_size\n",
        "        self.param.decoder_vocab_size=decoder_vocab_size\n",
        "        #Add Encoder layer\n",
        "\n",
        "        self.encoder = Encoder(self.param)\n",
        "\n",
        "        #Create decode with or without any attention layer\n",
        "        #Check following properties to add attention\n",
        "        # self.attention\n",
        "        # self.attention_type\n",
        "        self.decoder = Decoder(self.param)\n",
        "                \n",
        "\n",
        "    @tf.function\n",
        "    def train(self, input, target, enc_state):\n",
        "\n",
        "        loss = 0 \n",
        "\n",
        "        with tf.GradientTape() as tape: \n",
        "\n",
        "            enc_out, enc_state = self.encoder(input, enc_state)\n",
        "\n",
        "            dec_state = enc_state\n",
        "            dec_input = tf.expand_dims([self.targ_tokenizer.word_index[\"\\t\"]]*self.batch_size ,1)\n",
        "\n",
        "            apply_teacher_forcing=False\n",
        "            #decide whether to use teacher forcing\n",
        "            if random.random() < self.teacher_forcing_ratio:\n",
        "              apply_teacher_forcing=True\n",
        "            ## We use Teacher forcing to train the network\n",
        "            ## Each target at timestep t is passed as input for timestep t + 1\n",
        "            if  (apply_teacher_forcing==True):\n",
        "                #Apply teacher forcing\n",
        "                for t in range(1, target.shape[1]):\n",
        "\n",
        "                    preds, dec_state, _ = self.decoder(dec_input, dec_state, enc_out)\n",
        "                    loss += self.loss(target[:,t], preds)\n",
        "                    self.metric.update_state(target[:,t], preds)\n",
        "                    #As teacher forcing  applied we pass next target as decoder input\n",
        "                    dec_input = tf.expand_dims(target[:,t], 1)\n",
        "            \n",
        "            else:\n",
        "                #Without teacher forcing\n",
        "\n",
        "                for t in range(1, target.shape[1]):\n",
        "\n",
        "                    preds, dec_state, _ = self.decoder(dec_input, dec_state, enc_out)\n",
        "                    loss += self.loss(target[:,t], preds)\n",
        "                    self.metric.update_state(target[:,t], preds)\n",
        "                    #As teacher forcing not applied we pass decoder input as whatever we predict\n",
        "                    preds = tf.argmax(preds, 1)\n",
        "                    dec_input = tf.expand_dims(preds, 1)\n",
        "\n",
        "\n",
        "            batch_loss = loss / target.shape[1]\n",
        "\n",
        "            variables = self.encoder.variables + self.decoder.variables\n",
        "            gradients = tape.gradient(loss, variables)\n",
        "\n",
        "            self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "        return batch_loss, self.metric.result()\n",
        "\n",
        "    \n",
        "  \n",
        "\n",
        "    def fit(self, dataset, val_dataset, batch_size=128, epochs=5, wandb=None,apply_teacher_forcing=True, teacher_forcing_ratio=0.7):\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.apply_teacher_forcing = apply_teacher_forcing\n",
        "        self.teacher_forcing_ratio=teacher_forcing_ratio\n",
        "        #Prepare chunk of data based on batch size provided\n",
        "        steps_per_epoch = len(dataset) // self.batch_size\n",
        "        #steps_per_epoch_val = len(val_dataset) // self.batch_size\n",
        "        \n",
        "        dataset = dataset.batch(self.batch_size, drop_remainder=False)\n",
        "        #val_dataset = val_dataset.batch(self.batch_size, drop_remainder=False)\n",
        "\n",
        "      \n",
        "        sample_inp, sample_targ = next(iter(dataset))\n",
        "        self.max_target_len = sample_targ.shape[1]\n",
        "        self.max_input_len = sample_inp.shape[1]\n",
        "\n",
        "        #Store Encoder ,decoder details in case model get good accuracy\n",
        "        #Will be useful to restore best model\n",
        "        self.bestEncoder=self.encoder\n",
        "        self.bestDecoder=self.decoder\n",
        "        self.bestoptimizer=self.optimizer\n",
        "        \n",
        "        accuracyDegradePatience=0\n",
        "        self.oldaccuracy=0\n",
        "        for epoch in  tqdm(range(1, epochs+1), total = epochs,desc=\"Epochs \"):\n",
        "             \n",
        "            if(accuracyDegradePatience>=self.patience):\n",
        "                if(self.restoreBestModel==True):\n",
        "                    self.encoder=self.bestEncoder\n",
        "                    self.decoder=self.bestDecoder\n",
        "                    self.optimizer=self.bestoptimizer\n",
        "                break\n",
        "            ## Training loop ##\n",
        "            total_loss = 0\n",
        "            total_acc = 0\n",
        "            self.metric.reset_states()\n",
        "\n",
        "            starting_time = time.time()\n",
        "            enc_state = self.encoder.initialize_hidden_state(self.batch_size)\n",
        "\n",
        "            \n",
        "           \n",
        "            for batch, (input, target) in enumerate(dataset.take(steps_per_epoch)):\n",
        "                #Accumulate loss and accurecy for each batch\n",
        "                batch_loss, acc = self.train(input, target, enc_state)\n",
        "                total_loss += batch_loss\n",
        "                total_acc += acc\n",
        "            #Calculate validation accurecy for current Epoch\n",
        "          \n",
        "            avg_acc = total_acc / steps_per_epoch\n",
        "            avg_loss = total_loss / steps_per_epoch\n",
        "\n",
        "            # Validation loop ##\n",
        "            total_val_loss = 0\n",
        "            total_val_acc = 0\n",
        "            self.metric.reset_states()\n",
        "\n",
        "            enc_state = self.encoder.initialize_hidden_state(self.batch_size)\n",
        "\n",
        "            #Process data in batches\n",
        "            \n",
        "            avg_val_loss, avg_val_acc = self.evaluate(val_dataset,batch_size=self.batch_size)\n",
        "              \n",
        "\n",
        "           \n",
        "            #Verify if model performance degrading.add()\n",
        "            #In case train accurecy improved but no significant imprrovement in validation\n",
        "            #Add condition for early stopping\n",
        "            #Restore best model based on the input\n",
        "            if(self.oldaccuracy>avg_val_acc):\n",
        "              accuracyDegradePatience+=1\n",
        "            else:\n",
        "              self.bestEncoder=self.encoder\n",
        "              self.bestDecoder=self.decoder\n",
        "              self.bestoptimizer=self.optimizer\n",
        "              self.oldaccuracy=avg_val_acc\n",
        "              accuracyDegradePatience=0\n",
        "            print( \"\\nTrain Loss: {0:.4f} Train Accuracy: {1:.4f} Validation Loss: {2:.4f} Validation Accuracy: {3:.4f}\".format(avg_loss, avg_acc*100, avg_val_loss, avg_val_acc*100))\n",
        "            \n",
        "            time_taken = time.time() - starting_time\n",
        "\n",
        "            #Add logs for WanDb\n",
        "            self.stats.append({\"epoch\": epoch,\n",
        "                            \"train_loss\": avg_loss,\n",
        "                            \"val_loss\": avg_val_loss,\n",
        "                            \"train_acc\": avg_acc*100,\n",
        "                            \"val_acc\": avg_val_acc*100,\n",
        "                            \"training time\": time_taken})\n",
        "            \n",
        "            #Log to wanDB\n",
        "            if not (wandb is None):\n",
        "                wandb.log(self.stats[-1])\n",
        "            \n",
        "            print(f\"\\nTime taken for the epoch {time_taken:.4f}\")\n",
        "           \n",
        "        \n",
        "        print(\"\\nModel trained successfully !!\")\n",
        "    @tf.function\n",
        "    def validation(self, inp, trgt, encoder_state):\n",
        "        #Custom validation\n",
        "\n",
        "        loss = 0\n",
        "        #encoder input\n",
        "        encoder_output, encoder_state = self.encoder(inp, encoder_state)\n",
        "\n",
        "        #Set initial state of decoder from encoder state\n",
        "        decoder_state = encoder_state\n",
        "        decoder_input = tf.expand_dims([self.targ_tokenizer.word_index[\"\\t\"]]*self.batch_size ,1)\n",
        "\n",
        "        for t in range(1, trgt.shape[1]):\n",
        "            #Get decoder prediction\n",
        "            prediction, decoder_state, _ = self.decoder(decoder_input, decoder_state, encoder_output)\n",
        "            loss += self.loss(trgt[:,t], prediction)\n",
        "            self.metric.update_state(trgt[:,t], prediction)\n",
        "\n",
        "            prediction = tf.argmax(prediction, 1)\n",
        "            decoder_input = tf.expand_dims(prediction, 1)\n",
        "\n",
        "        batch_loss = loss / trgt.shape[1]\n",
        "        \n",
        "        return batch_loss, self.metric.result()    \n",
        "    def evaluate(self, test_dataset, batch_size=None):\n",
        "        \"\"\"Evaluate our model on test data\"\"\"\n",
        "        if batch_size is not None:\n",
        "            self.batch_size = batch_size\n",
        "\n",
        "        #prepare chuck of data based on the batch size\n",
        "        steps_per_epoch_test = len(test_dataset) // batch_size\n",
        "        test_dataset = test_dataset.batch(batch_size, drop_remainder=True)\n",
        "        \n",
        "        total_test_loss = 0\n",
        "        total_test_acc = 0\n",
        "        self.metric.reset_states()\n",
        "\n",
        "        enc_state = self.encoder.initialize_hidden_state(self.batch_size)\n",
        "\n",
        "        #print(\"\\nRunning test dataset through the model...\\n\")\n",
        "        #Run in batches based on the input batch size\n",
        "        for batch, (input, target) in enumerate(test_dataset.take(steps_per_epoch_test)):\n",
        "            batch_loss, acc = self.validation(input, target, enc_state)\n",
        "            total_test_loss += batch_loss\n",
        "            total_test_acc += acc\n",
        "\n",
        "        #Caculate avarage  test accuracy and loss\n",
        "        avg_test_acc = total_test_acc / steps_per_epoch_test\n",
        "        avg_test_loss = total_test_loss / steps_per_epoch_test\n",
        "\n",
        "        #Display details\n",
        "        #print(f\"Test Loss: {avg_test_loss:.4f} Test Accuracy: {avg_test_acc:.4f}\")\n",
        "\n",
        "        return avg_test_loss, avg_test_acc\n",
        "\n",
        "    \"\"\" This function used to translate english world to respective language\"\"\"\n",
        "\n",
        "    def translate(self, word, get_heatmap=False):\n",
        "        #start and end token for input word\n",
        "        start=\"\\t\"\n",
        "        end=\"\\n\"\n",
        "        word =start  + word + end\n",
        "\n",
        "        #Tokenize input and perform  preprocessing  \n",
        "        inputs = self.input_tokenizer.texts_to_sequences([word])\n",
        "        inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                               maxlen=self.max_input_len,\n",
        "                                                               padding=\"post\")\n",
        "\n",
        "        result = \"\"\n",
        "        att_wts = []\n",
        "\n",
        "        #Process input through encoder\n",
        "        enc_state = self.encoder.initialize_hidden_state(1)\n",
        "        enc_out, enc_state = self.encoder(inputs, enc_state)\n",
        "\n",
        "        # Set initial decoder sate to encoder state\n",
        "        dec_state = enc_state\n",
        "        dec_input = tf.expand_dims([self.targ_tokenizer.word_index[start]]*1, 1)\n",
        "\n",
        "        #Run the loop for maximum word size the target language can have\n",
        "        #We get this data during training \n",
        "        for t in range(1, self.max_target_len):\n",
        "\n",
        "            preds, dec_state, attention_weights = self.decoder(dec_input, dec_state, enc_out)\n",
        "            \n",
        "            #Add attention weights which is useful for generating attention heatmaps\n",
        "            if get_heatmap:\n",
        "                att_wts.append(attention_weights)\n",
        "            \n",
        "            #Pass the current prediction as input to next iteration\n",
        "            preds = tf.argmax(preds, 1)\n",
        "\n",
        "            #Accumulate target words\n",
        "            next_char = self.targ_tokenizer.index_word[preds.numpy().item()]\n",
        "            result += next_char\n",
        "\n",
        "            #Decoder input for next iteration\n",
        "            dec_input = tf.expand_dims(preds, 1)\n",
        "\n",
        "            #If we receive end token stop the loop\n",
        "            if next_char == end:\n",
        "                break\n",
        "\n",
        "        return result[:-1], att_wts[:-1]\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:08.103339Z",
          "iopub.status.busy": "2022-05-13T17:57:08.102802Z",
          "iopub.status.idle": "2022-05-13T17:57:08.113188Z",
          "shell.execute_reply": "2022-05-13T17:57:08.11221Z",
          "shell.execute_reply.started": "2022-05-13T17:57:08.103293Z"
        },
        "id": "rT-bKfIIWcES",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras import layers\n",
        "from tqdm import tqdm\n",
        "def get_layer(name, units, dropout, return_state=False, return_sequences=False):\n",
        "    \"\"\"Case when cell type is simple RNN\"\"\"\n",
        "    if name==\"rnn\":\n",
        "        return layers.SimpleRNN(units=units, dropout=dropout, \n",
        "                                return_state=return_state,\n",
        "                                return_sequences=return_sequences)\n",
        "        \n",
        "    \"\"\"Case when cell type is simple GRU\"\"\"\n",
        "    if name==\"gru\":\n",
        "        return layers.GRU(units=units, dropout=dropout, \n",
        "                          return_state=return_state,\n",
        "                          return_sequences=return_sequences)\n",
        "        \n",
        "    \"\"\"Case when cell type is simple LSTM\"\"\"\n",
        "    if name==\"lstm\":\n",
        "        return layers.LSTM(units=units, dropout=dropout, \n",
        "                           return_state=return_state,\n",
        "                           return_sequences=return_sequences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:08.115727Z",
          "iopub.status.busy": "2022-05-13T17:57:08.115041Z",
          "iopub.status.idle": "2022-05-13T17:57:08.125087Z",
          "shell.execute_reply": "2022-05-13T17:57:08.124108Z",
          "shell.execute_reply.started": "2022-05-13T17:57:08.115665Z"
        },
        "id": "6dS2JhY05vMR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import wandb "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:08.12831Z",
          "iopub.status.busy": "2022-05-13T17:57:08.12697Z",
          "iopub.status.idle": "2022-05-13T17:58:08.041134Z",
          "shell.execute_reply": "2022-05-13T17:58:08.039989Z",
          "shell.execute_reply.started": "2022-05-13T17:57:08.128261Z"
        },
        "id": "hLEZUp3W5wf9",
        "outputId": "6bfece5c-7d8a-4e60-ddfb-87fddbe880e5",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220514_171316-xfglkqln</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/kankan-jana/uncategorized/runs/xfglkqln\" target=\"_blank\">winter-wildflower-60</a></strong> to <a href=\"https://wandb.ai/kankan-jana/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/kankan-jana/uncategorized/runs/xfglkqln?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f0297a214d0>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCVhhqPMWkuS"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:58:08.044395Z",
          "iopub.status.busy": "2022-05-13T17:58:08.044039Z",
          "iopub.status.idle": "2022-05-13T17:58:09.136596Z",
          "shell.execute_reply": "2022-05-13T17:58:09.135546Z",
          "shell.execute_reply.started": "2022-05-13T17:58:08.044346Z"
        },
        "id": "p5Bt3Bf-_JhQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "START_TOKEN=\"\\t\"\n",
        "END_TOKEN=\"\\n\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWI9X48FjCPy",
        "outputId": "baff6fbb-89c8-481c-edb4-7698f75b7d22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "downloading....\n",
            "download Complete\n",
            "Extracting..\n",
            "Complete\n",
            "You are all set\n"
          ]
        }
      ],
      "source": [
        "downloadDataSet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:58:09.13871Z",
          "iopub.status.busy": "2022-05-13T17:58:09.138399Z",
          "iopub.status.idle": "2022-05-13T17:58:10.220961Z",
          "shell.execute_reply": "2022-05-13T17:58:10.219885Z",
          "shell.execute_reply.started": "2022-05-13T17:58:09.138662Z"
        },
        "id": "6ji0OxT0yxRA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "language=\"te\"\n",
        "train_dir, val_dir, test_dir = get_files(language)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:58:10.22341Z",
          "iopub.status.busy": "2022-05-13T17:58:10.222458Z",
          "iopub.status.idle": "2022-05-13T17:58:17.153887Z",
          "shell.execute_reply": "2022-05-13T17:58:17.152529Z",
          "shell.execute_reply.started": "2022-05-13T17:58:10.223364Z"
        },
        "id": "vWOhUaVWipOU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset, input_tokenizer, targ_tokenizer = preprocess_data(train_dir)\n",
        "val_dataset, _, _ = preprocess_data(val_dir,input_tokenizer,targ_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rOIv0WNAipOU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:58:17.156262Z",
          "iopub.status.busy": "2022-05-13T17:58:17.155897Z",
          "iopub.status.idle": "2022-05-13T17:58:20.361771Z",
          "shell.execute_reply": "2022-05-13T17:58:20.360733Z",
          "shell.execute_reply.started": "2022-05-13T17:58:17.156216Z"
        },
        "id": "A57aPT6P6uyg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#train data \n",
        "dataset, input_tokenizer, targ_tokenizer = preprocess_data(train_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:58:40.546067Z",
          "iopub.status.busy": "2022-05-13T17:58:40.545721Z",
          "iopub.status.idle": "2022-05-13T18:01:06.858305Z",
          "shell.execute_reply": "2022-05-13T18:01:06.854753Z",
          "shell.execute_reply.started": "2022-05-13T17:58:40.546033Z"
        },
        "id": "KZwjhjH0ipOV",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlbXxZXZjpEq"
      },
      "source": [
        "## Sweep Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-13T17:58:21.748889Z",
          "iopub.status.idle": "2022-05-13T17:58:21.749784Z",
          "shell.execute_reply": "2022-05-13T17:58:21.749484Z",
          "shell.execute_reply.started": "2022-05-13T17:58:21.749451Z"
        },
        "id": "ikX_8MejzT7-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "  \"name\": \"DL_Assignment3_Rnn\",\n",
        "  \"method\": \"bayes\",\n",
        "  \"metric\": {\n",
        "      \"name\": \"val_acc\",\n",
        "      \"goal\": \"maximize\",\n",
        "  },\n",
        "  \n",
        "  \"parameters\": {\n",
        "        \"num_of_encoders\":{\n",
        "          \"values\":[1,2,3]    \n",
        "        },\n",
        "        \"num_of_decoders\":{\n",
        "            \"values\":[1,2,3]      \n",
        "\n",
        "        },\n",
        "        \"cell_type\":{\n",
        "          \"values\":['lstm','gru']  \n",
        "        },\n",
        "        \n",
        "        \"lr\":{\n",
        "          \"values\":[0.001,0.005]  \n",
        "        },\n",
        "        \"optimizer\":{\n",
        "          \"values\":['adam','rmsprop']  \n",
        "        },\n",
        "        \"dropout\":{ \"values\": [0.3,0.5]},\n",
        "        \"latent_dim\":{ \"values\": [128,256,512]},\n",
        "        \"inp_emb_size\": {\"values\": [64,128,256]},\n",
        "     \n",
        "        \"batch_size\":{\"values\":[32,64,128]},\n",
        "        \n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-13T17:58:21.751564Z",
          "iopub.status.idle": "2022-05-13T17:58:21.752441Z",
          "shell.execute_reply": "2022-05-13T17:58:21.752161Z",
          "shell.execute_reply.started": "2022-05-13T17:58:21.752127Z"
        },
        "id": "2dt9LTQAzZHh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# This is the main function to use to train/fine-tune the model using wandb runs\n",
        "def train_wandb():\n",
        "    run = wandb.init()\n",
        "  \n",
        "    config=wandb.config\n",
        "    # Set the run name\n",
        "    name=\"num_of_encoders(\"+ str(config[\"num_of_encoders\"]) + \")_\"\n",
        "    name = \" num_of_decoders(\" + str(config[\"num_of_decoders\"]) + \")_\"\n",
        "    name += \" cell_type(\" + str(config[\"cell_type\"]) + \")_\"\n",
        "    \n",
        "    name += \"latent_dim(\" + str(config[\"latent_dim\"])+ \")_\"\n",
        "    name += \"lr(\" + str(config[\"lr\"])+ \")_\"\n",
        "    name += \"optimizer(\" + str(config[\"optimizer\"]) + \")_\"\n",
        "    name += \"dropout(\" + str(config[\"dropout\"]) + \")\"\n",
        "    name += \"inp_emb_size(\" + str(config[\"inp_emb_size\"]) + \")_\"\n",
        "    name+=\"batch_size(\" + str(config[\"batch_size\"]) + \")\"\n",
        "\n",
        "\n",
        "    wandb.run.name = name[:-1]\n",
        "    batch_size=config[\"batch_size\"]\n",
        "    inp_emb_size=config[\"inp_emb_size\"]\n",
        "    dropout=config[\"dropout\"]\n",
        "    optimizer=config[\"optimizer\"]\n",
        "    num_of_encoders=config[\"num_of_encoders\"]\n",
        "    num_of_decoders=config[\"num_of_decoders\"]\n",
        "\n",
        "    lr=config[\"lr\"]\n",
        "    latent_dim=config[\"latent_dim\"]\n",
        "    cell_type=config[\"cell_type\"]\n",
        "\n",
        "   \n",
        "    param=Parameters(language=\"te\",\\\n",
        "                        embedding_dim=inp_emb_size,\\\n",
        "                        encoder_layers=num_of_encoders,\\\n",
        "                        decoder_layers=num_of_decoders,\\\n",
        "                        layer_type=cell_type,\\\n",
        "                        units=latent_dim,\\\n",
        "                        dropout=dropout,\n",
        "                        epochs=15,\\\n",
        "                 batch_size=batch_size\\\n",
        "                   )\n",
        "    param.apply_teacher_forcing=True\n",
        "    param.teacher_forcing_ratio=1\n",
        "    param.patience=5\n",
        "    param.attention=True\n",
        "    model = SequenceTOSequence(param)\n",
        "    model.set_vocabulary(input_tokenizer, targ_tokenizer)\n",
        "\n",
        "    model.build(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\\\n",
        "                metric = tf.keras.metrics.SparseCategoricalAccuracy(),\\\n",
        "                optimizer = optimizer,\\\n",
        "                lr=lr\\\n",
        "                )\n",
        "\n",
        "    model.fit(dataset, val_dataset, epochs=param.epochs, wandb=wandb,teacher_forcing_ratio=param.teacher_forcing_ratio)       \n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2022-05-13T17:58:21.754176Z",
          "iopub.status.idle": "2022-05-13T17:58:21.755107Z",
          "shell.execute_reply": "2022-05-13T17:58:21.754787Z",
          "shell.execute_reply.started": "2022-05-13T17:58:21.754754Z"
        },
        "id": "kzkdsWRlZzTf",
        "outputId": "7bb2137c-1ffb-4e83-f5e5-ca0ebdfdb7c1",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: 8qmfawmk\n",
            "Sweep URL: https://wandb.ai/kankan-jana/CS6910_Assignment3/sweeps/8qmfawmk\n"
          ]
        }
      ],
      "source": [
        "           \n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, entity=\"kankan-jana\", project=\"CS6910_Assignment3\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "execution": {
          "iopub.status.busy": "2022-05-13T17:58:21.756822Z",
          "iopub.status.idle": "2022-05-13T17:58:21.757667Z",
          "shell.execute_reply": "2022-05-13T17:58:21.757398Z",
          "shell.execute_reply.started": "2022-05-13T17:58:21.757366Z"
        },
        "id": "LGxzAfZ42R_u",
        "outputId": "62208022-b40c-4fd8-b149-f83b046f0440",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4o0krqr8 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: gru\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinp_emb_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatent_dim: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_of_decoders: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_of_encoders: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220514_171540-4o0krqr8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/kankan-jana/CS6910_Assignment3/runs/4o0krqr8\" target=\"_blank\">pious-sweep-1</a></strong> to <a href=\"https://wandb.ai/kankan-jana/CS6910_Assignment3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/kankan-jana/CS6910_Assignment3/sweeps/8qmfawmk\" target=\"_blank\">https://wandb.ai/kankan-jana/CS6910_Assignment3/sweeps/8qmfawmk</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs :   0%|          | 0/15 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "\n",
        "wandb.agent(sweep_id, train_wandb)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "assignment-3-With_attention-sweep.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
